{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using GloVe Embeddings\n",
    "Machine Learning models cannot process text. Therefore, word embeddings are employed to convert the textual data into numerical data.\n",
    "\n",
    "#### Word Embeddings:\n",
    "A word embedding is a learned representation for text in an n-dimensional space where words that have the same meaning have a similar representation. This means that two similar words are represented by almost similar vectors that are very closely placed in a vector space. \n",
    "\n",
    "#### GloVe Word Embedding:\n",
    "Global Vectors for Words Representation(GloVe) is a method to create word embeddings by using matrix factorisation techinques on the word-context matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from os import path\n",
    "from scipy.stats import uniform\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from class_Word2Vec_vectorizer import Word2Vec_vectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignores warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Preparing  the GloVe pre-trained models \n",
    "\n",
    "- Download 100 and 200 dimensional vectors resulting from training on Wikipedia and Gigaword 5 data sets with 6 bilion tokens and a 400K word vocabulary from https://nlp.stanford.edu/projects/glove/. \n",
    "\n",
    "-  Convert the Glove file format to Word2Vec file format and load the models with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download\n",
    "glove_path = './GloVe'\n",
    "file_name = ['glove.6B.100d.txt', 'glove.6B.200d.txt', 'glove.6B.300d.txt']\n",
    "glove_input_file = [path.join(glove_path, f) for f in file_name]\n",
    "word2vec_output_file = [f + '.word2vec' for f in glove_input_file]\n",
    "\n",
    "# convert the GloVe file format to the word2vec file format\n",
    "for i in range(len(file_name)):\n",
    "    glove2word2vec(glove_input_file[i], word2vec_output_file[i])\n",
    "    \n",
    "# load the models \n",
    "GloVe_100 = KeyedVectors.load_word2vec_format(word2vec_output_file[0])\n",
    "GloVe_200 = KeyedVectors.load_word2vec_format(word2vec_output_file[1])\n",
    "#GloVe_300 = KeyedVectors.load_word2vec_format(word2vec_output_file[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation \n",
    "\n",
    "In this projects, we have 3 versions of text data processed from applying different preprocessing steps. For a given Article and for each version of text and each GloVe models, following steps were applied to find the optimal model: \n",
    "\n",
    "- For each classification models, \n",
    "\n",
    "1. Convert the text data into an average vector using a customised vectoriser. \n",
    "\n",
    "2. Scale the feature matrix using Min-Max scaler to normalise them.\n",
    "\n",
    "3. Select random combinations from a grid of hyperparameter values and train the model using 5-fold cross-validation. Find the optimal set of paramter which gives the highest accuracy. \n",
    "\n",
    "5. Refit the model with the best found parameters on the whole training set and store the parameter values and scores. \n",
    "\n",
    "- Compare the performance of models and find the best model.\n",
    "\n",
    "\n",
    "- Make predictions on test set using the best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance_random(X_train, y_train, embedding, parameters, folds, scoring):\n",
    "    # text list\n",
    "    ## standard(lowercase + removing numbers, punctuations) = without removing stopwords\n",
    "    processed_data = ['standard', 'stopwords_nltk', 'stopwords_spacy']\n",
    "    cols = ['Logistic', 'SVM', 'Naive_Bayes', 'LDA', 'QDA', 'Random_Forest','XGboost', 'Adaboost']\n",
    "    classifiers = [LogisticRegression(), SVC(), GaussianNB(), LinearDiscriminantAnalysis(),\n",
    "                  QuadraticDiscriminantAnalysis(), RandomForestClassifier(),\n",
    "                   XGBClassifier(objective= 'binary:logistic', use_label_encoder = False),\n",
    "                   AdaBoostClassifier()]\n",
    "    res = {}\n",
    "    for data in processed_data:\n",
    "        res[data] = {} # dict for each processed text\n",
    "        x_train = X_train[data] \n",
    "        for i in range(len(classifiers)):\n",
    "            vectorizer = Word2Vec_vectorizer(embedding)\n",
    "            classifier = classifiers[i]\n",
    "            pipeline = Pipeline([('vectorizer', vectorizer),\n",
    "                                       ('normalisation', MinMaxScaler()),\n",
    "                                       ('clf', classifier)])\n",
    "            random = RandomizedSearchCV(pipeline, parameters[cols[i]],  n_iter = 60, scoring = scoring,\n",
    "                                cv = folds, refit = 'accuracy', random_state = 1)\n",
    "            random.fit(x_train, y_train)\n",
    "            # saving the best performed model based on 'accuracy'\n",
    "            res[data].update({\n",
    "                cols[i]:{\n",
    "                    'random_search' : random,\n",
    "                    'classifier': random.best_estimator_,\n",
    "                    'best_score': random.best_score_,\n",
    "                    'best_params': random.best_params_,\n",
    "                    'accuracy': random.cv_results_['mean_test_accuracy'],\n",
    "                    'precision': random.cv_results_['mean_test_precision'],\n",
    "                    'recall': random.cv_results_['mean_test_recall'],\n",
    "                    'f1_score': random.cv_results_['mean_test_f1_score']\n",
    "                    }})\n",
    "    index_1 = processed_data * 4\n",
    "    index_1.sort()\n",
    "    index_2 = ['Accuracy', 'Precision', 'Recall', 'F1 score'] * 3\n",
    "    pd_index = pd.MultiIndex.from_arrays([index_1, index_2], \n",
    "                                         names = ['Processing_Type', 'Performance_Metrics'])\n",
    "   # create a data frame with the models performance metrics scores \n",
    "    score_table = pd.DataFrame(index = pd_index, columns = cols)\n",
    "    # find the best model's score of other performance metrices \n",
    "    for c in cols:\n",
    "        ## find index for best score \n",
    "        best_score_std = res['standard'][c]['best_score']\n",
    "        acc_std = res['standard'][c]['accuracy']\n",
    "        index_std = np.where(acc_std == best_score_std)[0][0] ## choose first element if duplicates\n",
    "        \n",
    "        best_score_nltk = res['stopwords_nltk'][c]['best_score']\n",
    "        acc_nltk = res['stopwords_nltk'][c]['accuracy']\n",
    "        index_nltk = np.where(acc_nltk == best_score_nltk)[0][0]\n",
    "        \n",
    "        best_score_sp = res['stopwords_spacy'][c]['best_score']\n",
    "        acc_sp = res['stopwords_spacy'][c]['accuracy']\n",
    "        index_sp = np.where(acc_sp == best_score_sp)[0][0]\n",
    "        \n",
    "        score_table[c] =[\n",
    "            best_score_std,\n",
    "            res['standard'][c]['precision'][index_std],\n",
    "            res['standard'][c]['recall'][index_std],\n",
    "            res['standard'][c]['f1_score'][index_std],\n",
    "            best_score_nltk,\n",
    "            res['stopwords_nltk'][c]['precision'][index_nltk],\n",
    "            res['stopwords_nltk'][c]['recall'][index_nltk],\n",
    "            res['stopwords_nltk'][c]['f1_score'][index_nltk],\n",
    "            best_score_sp,\n",
    "            res['stopwords_spacy'][c]['precision'][index_sp],\n",
    "            res['stopwords_spacy'][c]['recall'][index_sp],\n",
    "            res['stopwords_spacy'][c]['f1_score'][index_sp]\n",
    "        ]\n",
    "        # best model \n",
    "        score_table['Best_model'] = score_table[['Logistic', 'SVM', 'Naive_Bayes', 'LDA', 'QDA', 'Random_Forest', 'XGboost', 'Adaboost']].idxmax(axis=1)\n",
    "    \n",
    "    return res, score_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_performance(art_num, GloVe_dim, stop_word, classifier):\n",
    "    # loading the best model\n",
    "    res = pickle.load(open('models/model_art{}_{}.p'.format(art_num, GloVe_dim), 'rb'))\n",
    "    best_model = res[stop_word][classifier]['classifer']\n",
    "    # keep the best model\n",
    "    pickle.dump(best_model, open( \"./models/best_model_art{}.p\".format(art_num), \"wb\" ))\n",
    "    \n",
    "    # loading test set\n",
    "    art_df = pd.read_csv('./processed_text_df/test_article_{}.csv'.format(art_num))\n",
    "    test_x = art_df[stop_word]\n",
    "    test_y = art_df.label\n",
    "    \n",
    "    # predict\n",
    "    pred_y = best_model.predict(test_x)\n",
    "    acc = accuracy_score(test_y, pred_y)\n",
    "    pre = precision_score(test_y, pred_y)\n",
    "    rec = recall_score(test_y, pred_y)\n",
    "    f1 = f1_score(test_y, pred_y)\n",
    "    \n",
    "    # performance table\n",
    "    test_res = pd.DataFrame([[classifier, acc, pre, rec, f1]], columns = ['Best_model', 'Accuracy', 'Precision', 'Recall', 'F1_score'])\n",
    "    # save \n",
    "    test_res.to_csv('./models/art{}_test.csv'.format(art_num))\n",
    "    \n",
    "    return test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipline parameters \n",
    "parameters = { \n",
    "    'Logistic': {\n",
    "        'clf__penalty':['l1', 'l2', 'none'],\n",
    "        'clf__C': uniform(0.001, 100),\n",
    "        'clf__max_iter':uniform(100, 2000),\n",
    "        'clf__solver':['saga']\n",
    "    },\n",
    "    'SVM': {\n",
    "        'clf__C': uniform(0.001, 1000),\n",
    "        'clf__kernel': ['linear', 'rbf'],\n",
    "        'clf__gamma': uniform(0.001, 10)\n",
    "    },\n",
    "    'Naive_Bayes': {\n",
    "        'clf__var_smoothing':uniform(1e-15, 1e-2)\n",
    "    },\n",
    "    'LDA': {\n",
    "        'clf__solver': ['lsqr', 'eigen'],\n",
    "        'clf__shrinkage': uniform()\n",
    "     },\n",
    "    'QDA': {\n",
    "        'clf__reg_param': uniform()\n",
    "    },\n",
    "    'Random_Forest': {\n",
    "        'clf__n_estimators': [50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "        'clf__criterion': ['gini', 'entropy'],\n",
    "        'clf__max_features': np.arange(0, 101)\n",
    "    },\n",
    "    'XGboost': {\n",
    "        'clf__learning_rate': uniform(0.001, 0.3),\n",
    "        'clf__gamma': uniform(0, 10),\n",
    "        'clf__n_estimators': np.arange(100, 900),\n",
    "        'clf__max_depth': np.arange(2, 10),\n",
    "        'clf__min_child_weight': np.arange(1, 9),\n",
    "        'clf__colsample_bytree': uniform(0.3, 0.7),\n",
    "        'clf__subsample': uniform(0.2, 0.8),\n",
    "    },\n",
    "    'Adaboost': {\n",
    "        'clf__n_estimators':np.arange(50, 1000, 1)\n",
    "        ,\n",
    "        'clf__learning_rate': uniform(0, 1)\n",
    "    }\n",
    "}\n",
    "# define dictionary with performance metrics\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'f1_score': make_scorer(f1_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading train-test sets\n",
    "art2_train = pd.read_csv('./processed_text_df/train_article_2.csv')\n",
    "art2_test = pd.read_csv('./processed_text_df/test_article_2.csv')\n",
    "# labels\n",
    "train_y = art2_train.label\n",
    "test_y = art2_test.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100-Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processing_Type</th>\n",
       "      <th>Performance_Metrics</th>\n",
       "      <th>Logistic</th>\n",
       "      <th>SVM</th>\n",
       "      <th>Naive_Bayes</th>\n",
       "      <th>LDA</th>\n",
       "      <th>QDA</th>\n",
       "      <th>Random_Forest</th>\n",
       "      <th>XGboost</th>\n",
       "      <th>Adaboost</th>\n",
       "      <th>Best_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>standard</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.747312</td>\n",
       "      <td>0.759785</td>\n",
       "      <td>0.591828</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>0.760860</td>\n",
       "      <td>0.740215</td>\n",
       "      <td>0.740430</td>\n",
       "      <td>0.747312</td>\n",
       "      <td>LDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>standard</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.739955</td>\n",
       "      <td>0.761849</td>\n",
       "      <td>0.655462</td>\n",
       "      <td>0.763611</td>\n",
       "      <td>0.773007</td>\n",
       "      <td>0.730247</td>\n",
       "      <td>0.713684</td>\n",
       "      <td>0.741865</td>\n",
       "      <td>QDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>standard</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.765000</td>\n",
       "      <td>0.750833</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.797500</td>\n",
       "      <td>0.735833</td>\n",
       "      <td>0.829167</td>\n",
       "      <td>0.751667</td>\n",
       "      <td>XGboost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>standard</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.749704</td>\n",
       "      <td>0.753136</td>\n",
       "      <td>0.390169</td>\n",
       "      <td>0.776947</td>\n",
       "      <td>0.764777</td>\n",
       "      <td>0.728424</td>\n",
       "      <td>0.760100</td>\n",
       "      <td>0.743867</td>\n",
       "      <td>LDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.753548</td>\n",
       "      <td>0.734194</td>\n",
       "      <td>0.681935</td>\n",
       "      <td>0.773548</td>\n",
       "      <td>0.741505</td>\n",
       "      <td>0.772903</td>\n",
       "      <td>0.773548</td>\n",
       "      <td>0.791828</td>\n",
       "      <td>Adaboost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.746275</td>\n",
       "      <td>0.752843</td>\n",
       "      <td>0.751667</td>\n",
       "      <td>0.716370</td>\n",
       "      <td>0.776123</td>\n",
       "      <td>0.761328</td>\n",
       "      <td>0.782288</td>\n",
       "      <td>Adaboost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.763333</td>\n",
       "      <td>0.699167</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.803333</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>0.764167</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.802500</td>\n",
       "      <td>QDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.748698</td>\n",
       "      <td>0.719428</td>\n",
       "      <td>0.636428</td>\n",
       "      <td>0.772957</td>\n",
       "      <td>0.760904</td>\n",
       "      <td>0.765001</td>\n",
       "      <td>0.778735</td>\n",
       "      <td>0.789091</td>\n",
       "      <td>Adaboost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.753763</td>\n",
       "      <td>0.766882</td>\n",
       "      <td>0.714624</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>0.760645</td>\n",
       "      <td>0.785806</td>\n",
       "      <td>0.785806</td>\n",
       "      <td>0.799355</td>\n",
       "      <td>Adaboost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.747604</td>\n",
       "      <td>0.781951</td>\n",
       "      <td>0.780375</td>\n",
       "      <td>0.759649</td>\n",
       "      <td>0.723810</td>\n",
       "      <td>0.778926</td>\n",
       "      <td>0.779035</td>\n",
       "      <td>0.787267</td>\n",
       "      <td>Adaboost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.764167</td>\n",
       "      <td>0.724167</td>\n",
       "      <td>0.611667</td>\n",
       "      <td>0.803333</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.789167</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.817500</td>\n",
       "      <td>QDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.751099</td>\n",
       "      <td>0.749249</td>\n",
       "      <td>0.679766</td>\n",
       "      <td>0.776426</td>\n",
       "      <td>0.777056</td>\n",
       "      <td>0.781965</td>\n",
       "      <td>0.780118</td>\n",
       "      <td>0.796369</td>\n",
       "      <td>Adaboost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Processing_Type Performance_Metrics  Logistic       SVM  Naive_Bayes  \\\n",
       "0          standard            Accuracy  0.747312  0.759785     0.591828   \n",
       "1          standard           Precision  0.739955  0.761849     0.655462   \n",
       "2          standard              Recall  0.765000  0.750833     0.344167   \n",
       "3          standard            F1 score  0.749704  0.753136     0.390169   \n",
       "4    stopwords_nltk            Accuracy  0.753548  0.734194     0.681935   \n",
       "5    stopwords_nltk           Precision  0.745455  0.746275     0.752843   \n",
       "6    stopwords_nltk              Recall  0.763333  0.699167     0.560000   \n",
       "7    stopwords_nltk            F1 score  0.748698  0.719428     0.636428   \n",
       "8   stopwords_spacy            Accuracy  0.753763  0.766882     0.714624   \n",
       "9   stopwords_spacy           Precision  0.747604  0.781951     0.780375   \n",
       "10  stopwords_spacy              Recall  0.764167  0.724167     0.611667   \n",
       "11  stopwords_spacy            F1 score  0.751099  0.749249     0.679766   \n",
       "\n",
       "         LDA       QDA  Random_Forest   XGboost  Adaboost Best_model  \n",
       "0   0.773333  0.760860       0.740215  0.740430  0.747312        LDA  \n",
       "1   0.763611  0.773007       0.730247  0.713684  0.741865        QDA  \n",
       "2   0.791667  0.797500       0.735833  0.829167  0.751667    XGboost  \n",
       "3   0.776947  0.764777       0.728424  0.760100  0.743867        LDA  \n",
       "4   0.773548  0.741505       0.772903  0.773548  0.791828   Adaboost  \n",
       "5   0.751667  0.716370       0.776123  0.761328  0.782288   Adaboost  \n",
       "6   0.803333  0.835000       0.764167  0.805000  0.802500        QDA  \n",
       "7   0.772957  0.760904       0.765001  0.778735  0.789091   Adaboost  \n",
       "8   0.773333  0.760645       0.785806  0.785806  0.799355   Adaboost  \n",
       "9   0.759649  0.723810       0.778926  0.779035  0.787267   Adaboost  \n",
       "10  0.803333  0.860000       0.789167  0.790000  0.817500        QDA  \n",
       "11  0.776426  0.777056       0.781965  0.780118  0.796369   Adaboost  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_art2_100, art2_perform_100 = evaluate_performance_random(art2_train, train_y, GloVe_100, parameters, 5, scoring)\n",
    "pickle.dump(res_art2_100, open( \"./models/model_art2_100.p\", \"wb\" ))\n",
    "art2_perform_100.to_csv('./models/art2_100.csv')\n",
    "art2_perform_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 200-Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processing_Type</th>\n",
       "      <th>Performance_Metrics</th>\n",
       "      <th>Logistic</th>\n",
       "      <th>SVM</th>\n",
       "      <th>Naive_Bayes</th>\n",
       "      <th>LDA</th>\n",
       "      <th>QDA</th>\n",
       "      <th>Random_Forest</th>\n",
       "      <th>XGboost</th>\n",
       "      <th>Adaboost</th>\n",
       "      <th>Best_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>standard</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.747097</td>\n",
       "      <td>0.708387</td>\n",
       "      <td>0.578495</td>\n",
       "      <td>0.734194</td>\n",
       "      <td>0.700860</td>\n",
       "      <td>0.799140</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.779355</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>standard</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.739455</td>\n",
       "      <td>0.718158</td>\n",
       "      <td>0.645201</td>\n",
       "      <td>0.717778</td>\n",
       "      <td>0.728419</td>\n",
       "      <td>0.795626</td>\n",
       "      <td>0.746754</td>\n",
       "      <td>0.778332</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>standard</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.671667</td>\n",
       "      <td>0.358333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.675833</td>\n",
       "      <td>0.804167</td>\n",
       "      <td>0.776667</td>\n",
       "      <td>0.777500</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>standard</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.739274</td>\n",
       "      <td>0.690019</td>\n",
       "      <td>0.378023</td>\n",
       "      <td>0.729978</td>\n",
       "      <td>0.697042</td>\n",
       "      <td>0.796925</td>\n",
       "      <td>0.760061</td>\n",
       "      <td>0.772421</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.727957</td>\n",
       "      <td>0.701505</td>\n",
       "      <td>0.766237</td>\n",
       "      <td>0.760645</td>\n",
       "      <td>0.785806</td>\n",
       "      <td>0.773548</td>\n",
       "      <td>0.753763</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.747421</td>\n",
       "      <td>0.716053</td>\n",
       "      <td>0.758939</td>\n",
       "      <td>0.759524</td>\n",
       "      <td>0.727843</td>\n",
       "      <td>0.784866</td>\n",
       "      <td>0.760194</td>\n",
       "      <td>0.754769</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.789167</td>\n",
       "      <td>0.751667</td>\n",
       "      <td>0.613333</td>\n",
       "      <td>0.776667</td>\n",
       "      <td>0.832500</td>\n",
       "      <td>0.777500</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>QDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.762877</td>\n",
       "      <td>0.730484</td>\n",
       "      <td>0.671141</td>\n",
       "      <td>0.762869</td>\n",
       "      <td>0.774764</td>\n",
       "      <td>0.777073</td>\n",
       "      <td>0.773922</td>\n",
       "      <td>0.752850</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.773118</td>\n",
       "      <td>0.740645</td>\n",
       "      <td>0.727527</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.740645</td>\n",
       "      <td>0.773548</td>\n",
       "      <td>0.766882</td>\n",
       "      <td>0.753763</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.772500</td>\n",
       "      <td>0.730947</td>\n",
       "      <td>0.778409</td>\n",
       "      <td>0.761836</td>\n",
       "      <td>0.700212</td>\n",
       "      <td>0.774447</td>\n",
       "      <td>0.765738</td>\n",
       "      <td>0.754726</td>\n",
       "      <td>Naive_Bayes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.776667</td>\n",
       "      <td>0.778333</td>\n",
       "      <td>0.652500</td>\n",
       "      <td>0.776667</td>\n",
       "      <td>0.859167</td>\n",
       "      <td>0.779167</td>\n",
       "      <td>0.777500</td>\n",
       "      <td>0.750833</td>\n",
       "      <td>QDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.768178</td>\n",
       "      <td>0.752366</td>\n",
       "      <td>0.702472</td>\n",
       "      <td>0.765892</td>\n",
       "      <td>0.764145</td>\n",
       "      <td>0.773522</td>\n",
       "      <td>0.763938</td>\n",
       "      <td>0.747382</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Processing_Type Performance_Metrics  Logistic       SVM  Naive_Bayes  \\\n",
       "0          standard            Accuracy  0.747097  0.708387     0.578495   \n",
       "1          standard           Precision  0.739455  0.718158     0.645201   \n",
       "2          standard              Recall  0.750000  0.671667     0.358333   \n",
       "3          standard            F1 score  0.739274  0.690019     0.378023   \n",
       "4    stopwords_nltk            Accuracy  0.766667  0.727957     0.701505   \n",
       "5    stopwords_nltk           Precision  0.747421  0.716053     0.758939   \n",
       "6    stopwords_nltk              Recall  0.789167  0.751667     0.613333   \n",
       "7    stopwords_nltk            F1 score  0.762877  0.730484     0.671141   \n",
       "8   stopwords_spacy            Accuracy  0.773118  0.740645     0.727527   \n",
       "9   stopwords_spacy           Precision  0.772500  0.730947     0.778409   \n",
       "10  stopwords_spacy              Recall  0.776667  0.778333     0.652500   \n",
       "11  stopwords_spacy            F1 score  0.768178  0.752366     0.702472   \n",
       "\n",
       "         LDA       QDA  Random_Forest   XGboost  Adaboost     Best_model  \n",
       "0   0.734194  0.700860       0.799140  0.760000  0.779355  Random_Forest  \n",
       "1   0.717778  0.728419       0.795626  0.746754  0.778332  Random_Forest  \n",
       "2   0.750000  0.675833       0.804167  0.776667  0.777500  Random_Forest  \n",
       "3   0.729978  0.697042       0.796925  0.760061  0.772421  Random_Forest  \n",
       "4   0.766237  0.760645       0.785806  0.773548  0.753763  Random_Forest  \n",
       "5   0.759524  0.727843       0.784866  0.760194  0.754769  Random_Forest  \n",
       "6   0.776667  0.832500       0.777500  0.791667  0.753333            QDA  \n",
       "7   0.762869  0.774764       0.777073  0.773922  0.752850  Random_Forest  \n",
       "8   0.766667  0.740645       0.773548  0.766882  0.753763  Random_Forest  \n",
       "9   0.761836  0.700212       0.774447  0.765738  0.754726    Naive_Bayes  \n",
       "10  0.776667  0.859167       0.779167  0.777500  0.750833            QDA  \n",
       "11  0.765892  0.764145       0.773522  0.763938  0.747382  Random_Forest  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_art2_200, art2_perform_200 = evaluate_performance_random(art2_train, train_y, GloVe_200, parameters, 5, scoring)\n",
    "pickle.dump(res_art2_200, open( \"./models/model_art2_200.p\", \"wb\" ))\n",
    "art2_perform_200.to_csv('./models/art2_200.csv')\n",
    "\n",
    "art2_perform_200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Performance of the best model for Article 2 \n",
    "\n",
    "- Adaboost\n",
    "\n",
    "- 100 dimensional GloVe\n",
    "\n",
    "- Stop words with SpaCy \n",
    "\n",
    "- Validation score: 0.799355"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.701493</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.811321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall  F1_score\n",
       "0  0.701493   0.895833  0.741379  0.811321"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art2_test = test_performance('2', '100', 'stopwords_spacy', 'Adaboost')\n",
    "art2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading train set\n",
    "art3_train = pd.read_csv('./processed_text_df/train_article_3.csv')\n",
    "# labels\n",
    "train_y = art3_train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100d\n",
    "res_art3_100, art3_perform_100 = evaluate_performance_random(art3_train, train_y, GloVe_100, parameters, 5, scoring)\n",
    "pickle.dump(res_art3_100, open( \"./models/model_art3_100.p\", \"wb\" ))\n",
    "art3_perform_100.to_csv('./models/art3_100.csv')\n",
    "\n",
    "# 200d\n",
    "res_art3_200, art3_perform_200 = evaluate_performance_random(art3_train, train_y, GloVe_200, parameters, 5, scoring)\n",
    "pickle.dump(res_art3_200, open( \"./models/model_art3_200.p\", \"wb\" ))\n",
    "art3_perform_200.to_csv('./models/art3_200.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processing_Type</th>\n",
       "      <th>Performance_Metrics</th>\n",
       "      <th>Logistic</th>\n",
       "      <th>SVM</th>\n",
       "      <th>Naive_Bayes</th>\n",
       "      <th>LDA</th>\n",
       "      <th>QDA</th>\n",
       "      <th>Random_Forest</th>\n",
       "      <th>XGboost</th>\n",
       "      <th>Adaboost</th>\n",
       "      <th>Best_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>standard</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.740243</td>\n",
       "      <td>0.725120</td>\n",
       "      <td>0.620178</td>\n",
       "      <td>0.748776</td>\n",
       "      <td>0.742393</td>\n",
       "      <td>0.770350</td>\n",
       "      <td>0.751041</td>\n",
       "      <td>0.733814</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>standard</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.736733</td>\n",
       "      <td>0.715071</td>\n",
       "      <td>0.576878</td>\n",
       "      <td>0.748428</td>\n",
       "      <td>0.740278</td>\n",
       "      <td>0.764326</td>\n",
       "      <td>0.744214</td>\n",
       "      <td>0.726139</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>standard</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.750416</td>\n",
       "      <td>0.750231</td>\n",
       "      <td>0.961055</td>\n",
       "      <td>0.750231</td>\n",
       "      <td>0.750416</td>\n",
       "      <td>0.780851</td>\n",
       "      <td>0.763460</td>\n",
       "      <td>0.750694</td>\n",
       "      <td>Naive_Bayes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>standard</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.741746</td>\n",
       "      <td>0.729315</td>\n",
       "      <td>0.718970</td>\n",
       "      <td>0.746912</td>\n",
       "      <td>0.743969</td>\n",
       "      <td>0.772237</td>\n",
       "      <td>0.752823</td>\n",
       "      <td>0.737847</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.744498</td>\n",
       "      <td>0.738092</td>\n",
       "      <td>0.710181</td>\n",
       "      <td>0.757355</td>\n",
       "      <td>0.729444</td>\n",
       "      <td>0.757493</td>\n",
       "      <td>0.748936</td>\n",
       "      <td>0.731663</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.734131</td>\n",
       "      <td>0.732345</td>\n",
       "      <td>0.662082</td>\n",
       "      <td>0.750207</td>\n",
       "      <td>0.685998</td>\n",
       "      <td>0.757029</td>\n",
       "      <td>0.759527</td>\n",
       "      <td>0.735621</td>\n",
       "      <td>XGboost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.759019</td>\n",
       "      <td>0.746161</td>\n",
       "      <td>0.888067</td>\n",
       "      <td>0.767715</td>\n",
       "      <td>0.853747</td>\n",
       "      <td>0.754949</td>\n",
       "      <td>0.729140</td>\n",
       "      <td>0.729325</td>\n",
       "      <td>Naive_Bayes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.743998</td>\n",
       "      <td>0.738824</td>\n",
       "      <td>0.756136</td>\n",
       "      <td>0.757352</td>\n",
       "      <td>0.759837</td>\n",
       "      <td>0.754841</td>\n",
       "      <td>0.743576</td>\n",
       "      <td>0.731217</td>\n",
       "      <td>QDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.744475</td>\n",
       "      <td>0.744475</td>\n",
       "      <td>0.708076</td>\n",
       "      <td>0.750904</td>\n",
       "      <td>0.718783</td>\n",
       "      <td>0.748868</td>\n",
       "      <td>0.755228</td>\n",
       "      <td>0.733860</td>\n",
       "      <td>XGboost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.737892</td>\n",
       "      <td>0.742286</td>\n",
       "      <td>0.661995</td>\n",
       "      <td>0.745280</td>\n",
       "      <td>0.710746</td>\n",
       "      <td>0.746720</td>\n",
       "      <td>0.764494</td>\n",
       "      <td>0.732920</td>\n",
       "      <td>XGboost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.754857</td>\n",
       "      <td>0.746253</td>\n",
       "      <td>0.875393</td>\n",
       "      <td>0.754672</td>\n",
       "      <td>0.737835</td>\n",
       "      <td>0.750694</td>\n",
       "      <td>0.737835</td>\n",
       "      <td>0.742461</td>\n",
       "      <td>Naive_Bayes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.745462</td>\n",
       "      <td>0.743809</td>\n",
       "      <td>0.752290</td>\n",
       "      <td>0.747000</td>\n",
       "      <td>0.723701</td>\n",
       "      <td>0.747668</td>\n",
       "      <td>0.749194</td>\n",
       "      <td>0.736621</td>\n",
       "      <td>Naive_Bayes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Processing_Type Performance_Metrics  Logistic       SVM  Naive_Bayes  \\\n",
       "0          standard            Accuracy  0.740243  0.725120     0.620178   \n",
       "1          standard           Precision  0.736733  0.715071     0.576878   \n",
       "2          standard              Recall  0.750416  0.750231     0.961055   \n",
       "3          standard            F1 score  0.741746  0.729315     0.718970   \n",
       "4    stopwords_nltk            Accuracy  0.744498  0.738092     0.710181   \n",
       "5    stopwords_nltk           Precision  0.734131  0.732345     0.662082   \n",
       "6    stopwords_nltk              Recall  0.759019  0.746161     0.888067   \n",
       "7    stopwords_nltk            F1 score  0.743998  0.738824     0.756136   \n",
       "8   stopwords_spacy            Accuracy  0.744475  0.744475     0.708076   \n",
       "9   stopwords_spacy           Precision  0.737892  0.742286     0.661995   \n",
       "10  stopwords_spacy              Recall  0.754857  0.746253     0.875393   \n",
       "11  stopwords_spacy            F1 score  0.745462  0.743809     0.752290   \n",
       "\n",
       "         LDA       QDA  Random_Forest   XGboost  Adaboost     Best_model  \n",
       "0   0.748776  0.742393       0.770350  0.751041  0.733814  Random_Forest  \n",
       "1   0.748428  0.740278       0.764326  0.744214  0.726139  Random_Forest  \n",
       "2   0.750231  0.750416       0.780851  0.763460  0.750694    Naive_Bayes  \n",
       "3   0.746912  0.743969       0.772237  0.752823  0.737847  Random_Forest  \n",
       "4   0.757355  0.729444       0.757493  0.748936  0.731663  Random_Forest  \n",
       "5   0.750207  0.685998       0.757029  0.759527  0.735621        XGboost  \n",
       "6   0.767715  0.853747       0.754949  0.729140  0.729325    Naive_Bayes  \n",
       "7   0.757352  0.759837       0.754841  0.743576  0.731217            QDA  \n",
       "8   0.750904  0.718783       0.748868  0.755228  0.733860        XGboost  \n",
       "9   0.745280  0.710746       0.746720  0.764494  0.732920        XGboost  \n",
       "10  0.754672  0.737835       0.750694  0.737835  0.742461    Naive_Bayes  \n",
       "11  0.747000  0.723701       0.747668  0.749194  0.736621    Naive_Bayes  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art3_perform_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processing_Type</th>\n",
       "      <th>Performance_Metrics</th>\n",
       "      <th>Logistic</th>\n",
       "      <th>SVM</th>\n",
       "      <th>Naive_Bayes</th>\n",
       "      <th>LDA</th>\n",
       "      <th>QDA</th>\n",
       "      <th>Random_Forest</th>\n",
       "      <th>XGboost</th>\n",
       "      <th>Adaboost</th>\n",
       "      <th>Best_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>standard</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.746694</td>\n",
       "      <td>0.742439</td>\n",
       "      <td>0.624411</td>\n",
       "      <td>0.768131</td>\n",
       "      <td>0.748845</td>\n",
       "      <td>0.765980</td>\n",
       "      <td>0.757401</td>\n",
       "      <td>0.742302</td>\n",
       "      <td>LDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>standard</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.742118</td>\n",
       "      <td>0.737679</td>\n",
       "      <td>0.580332</td>\n",
       "      <td>0.757100</td>\n",
       "      <td>0.723835</td>\n",
       "      <td>0.757670</td>\n",
       "      <td>0.758505</td>\n",
       "      <td>0.752070</td>\n",
       "      <td>XGboost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>standard</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.759019</td>\n",
       "      <td>0.758927</td>\n",
       "      <td>0.948011</td>\n",
       "      <td>0.789177</td>\n",
       "      <td>0.820074</td>\n",
       "      <td>0.780759</td>\n",
       "      <td>0.759019</td>\n",
       "      <td>0.720537</td>\n",
       "      <td>Naive_Bayes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>standard</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.748302</td>\n",
       "      <td>0.746255</td>\n",
       "      <td>0.717893</td>\n",
       "      <td>0.771554</td>\n",
       "      <td>0.766731</td>\n",
       "      <td>0.768107</td>\n",
       "      <td>0.757042</td>\n",
       "      <td>0.734848</td>\n",
       "      <td>LDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.750949</td>\n",
       "      <td>0.720956</td>\n",
       "      <td>0.712263</td>\n",
       "      <td>0.770281</td>\n",
       "      <td>0.753169</td>\n",
       "      <td>0.763921</td>\n",
       "      <td>0.763876</td>\n",
       "      <td>0.742416</td>\n",
       "      <td>LDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.745022</td>\n",
       "      <td>0.718719</td>\n",
       "      <td>0.664770</td>\n",
       "      <td>0.755908</td>\n",
       "      <td>0.709006</td>\n",
       "      <td>0.754580</td>\n",
       "      <td>0.757070</td>\n",
       "      <td>0.744356</td>\n",
       "      <td>XGboost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.763367</td>\n",
       "      <td>0.733302</td>\n",
       "      <td>0.879741</td>\n",
       "      <td>0.793617</td>\n",
       "      <td>0.871600</td>\n",
       "      <td>0.781036</td>\n",
       "      <td>0.776503</td>\n",
       "      <td>0.746161</td>\n",
       "      <td>Naive_Bayes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.752490</td>\n",
       "      <td>0.723677</td>\n",
       "      <td>0.756021</td>\n",
       "      <td>0.772399</td>\n",
       "      <td>0.779323</td>\n",
       "      <td>0.766441</td>\n",
       "      <td>0.765569</td>\n",
       "      <td>0.742309</td>\n",
       "      <td>QDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.757401</td>\n",
       "      <td>0.718852</td>\n",
       "      <td>0.701579</td>\n",
       "      <td>0.768108</td>\n",
       "      <td>0.742393</td>\n",
       "      <td>0.761748</td>\n",
       "      <td>0.750972</td>\n",
       "      <td>0.736033</td>\n",
       "      <td>LDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.753600</td>\n",
       "      <td>0.715320</td>\n",
       "      <td>0.657692</td>\n",
       "      <td>0.757589</td>\n",
       "      <td>0.691353</td>\n",
       "      <td>0.763674</td>\n",
       "      <td>0.742737</td>\n",
       "      <td>0.738807</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.763367</td>\n",
       "      <td>0.725162</td>\n",
       "      <td>0.866883</td>\n",
       "      <td>0.784829</td>\n",
       "      <td>0.888807</td>\n",
       "      <td>0.750786</td>\n",
       "      <td>0.763552</td>\n",
       "      <td>0.733580</td>\n",
       "      <td>QDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.757141</td>\n",
       "      <td>0.720025</td>\n",
       "      <td>0.746536</td>\n",
       "      <td>0.769192</td>\n",
       "      <td>0.774829</td>\n",
       "      <td>0.755404</td>\n",
       "      <td>0.752136</td>\n",
       "      <td>0.734341</td>\n",
       "      <td>QDA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Processing_Type Performance_Metrics  Logistic       SVM  Naive_Bayes  \\\n",
       "0          standard            Accuracy  0.746694  0.742439     0.624411   \n",
       "1          standard           Precision  0.742118  0.737679     0.580332   \n",
       "2          standard              Recall  0.759019  0.758927     0.948011   \n",
       "3          standard            F1 score  0.748302  0.746255     0.717893   \n",
       "4    stopwords_nltk            Accuracy  0.750949  0.720956     0.712263   \n",
       "5    stopwords_nltk           Precision  0.745022  0.718719     0.664770   \n",
       "6    stopwords_nltk              Recall  0.763367  0.733302     0.879741   \n",
       "7    stopwords_nltk            F1 score  0.752490  0.723677     0.756021   \n",
       "8   stopwords_spacy            Accuracy  0.757401  0.718852     0.701579   \n",
       "9   stopwords_spacy           Precision  0.753600  0.715320     0.657692   \n",
       "10  stopwords_spacy              Recall  0.763367  0.725162     0.866883   \n",
       "11  stopwords_spacy            F1 score  0.757141  0.720025     0.746536   \n",
       "\n",
       "         LDA       QDA  Random_Forest   XGboost  Adaboost     Best_model  \n",
       "0   0.768131  0.748845       0.765980  0.757401  0.742302            LDA  \n",
       "1   0.757100  0.723835       0.757670  0.758505  0.752070        XGboost  \n",
       "2   0.789177  0.820074       0.780759  0.759019  0.720537    Naive_Bayes  \n",
       "3   0.771554  0.766731       0.768107  0.757042  0.734848            LDA  \n",
       "4   0.770281  0.753169       0.763921  0.763876  0.742416            LDA  \n",
       "5   0.755908  0.709006       0.754580  0.757070  0.744356        XGboost  \n",
       "6   0.793617  0.871600       0.781036  0.776503  0.746161    Naive_Bayes  \n",
       "7   0.772399  0.779323       0.766441  0.765569  0.742309            QDA  \n",
       "8   0.768108  0.742393       0.761748  0.750972  0.736033            LDA  \n",
       "9   0.757589  0.691353       0.763674  0.742737  0.738807  Random_Forest  \n",
       "10  0.784829  0.888807       0.750786  0.763552  0.733580            QDA  \n",
       "11  0.769192  0.774829       0.755404  0.752136  0.734341            QDA  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art3_perform_200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Performance of the best model for Article 3\n",
    "\n",
    "- Random Forest\n",
    "\n",
    "- 100 dimensional GloVe\n",
    "\n",
    "- No stop words removed\n",
    "\n",
    "- Validation score: 0.770350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.730392</td>\n",
       "      <td>0.948905</td>\n",
       "      <td>0.730337</td>\n",
       "      <td>0.825397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall  F1_score\n",
       "0  0.730392   0.948905  0.730337  0.825397"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art3_test = test_performance('3', '100', 'standard', 'Random_Forest')\n",
    "art3_test                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading train set\n",
    "art5_train = pd.read_csv('./processed_text_df/train_article_5.csv')\n",
    "# labels\n",
    "train_y = art5_train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100d\n",
    "res_art5_100, art5_perform_100 = evaluate_performance_random(art5_train, train_y, GloVe_100, parameters, 5, scoring)\n",
    "pickle.dump(res_art5_100, open( \"./models/model_art5_100.p\", \"wb\" ))\n",
    "art5_perform_100.to_csv('./models/art5_100.csv')\n",
    "\n",
    "# 200d\n",
    "res_art5_200, art5_perform_200 = evaluate_performance_random(art5_train, train_y, GloVe_200, parameters, 5, scoring)\n",
    "pickle.dump(res_art5_200, open( \"./models/model_art5_200.p\", \"wb\" ))\n",
    "art5_perform_200.to_csv('./models/art5_200.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processing_Type</th>\n",
       "      <th>Performance_Metrics</th>\n",
       "      <th>Logistic</th>\n",
       "      <th>SVM</th>\n",
       "      <th>Naive_Bayes</th>\n",
       "      <th>LDA</th>\n",
       "      <th>QDA</th>\n",
       "      <th>Random_Forest</th>\n",
       "      <th>XGboost</th>\n",
       "      <th>Adaboost</th>\n",
       "      <th>Best_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>standard</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.751415</td>\n",
       "      <td>0.730636</td>\n",
       "      <td>0.639927</td>\n",
       "      <td>0.728139</td>\n",
       "      <td>0.710057</td>\n",
       "      <td>0.759207</td>\n",
       "      <td>0.740992</td>\n",
       "      <td>0.751315</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>standard</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.757197</td>\n",
       "      <td>0.737140</td>\n",
       "      <td>0.764118</td>\n",
       "      <td>0.749784</td>\n",
       "      <td>0.732146</td>\n",
       "      <td>0.768149</td>\n",
       "      <td>0.747541</td>\n",
       "      <td>0.767877</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>standard</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.746694</td>\n",
       "      <td>0.720513</td>\n",
       "      <td>0.404588</td>\n",
       "      <td>0.689474</td>\n",
       "      <td>0.658570</td>\n",
       "      <td>0.751552</td>\n",
       "      <td>0.735223</td>\n",
       "      <td>0.725101</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>standard</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.749844</td>\n",
       "      <td>0.726904</td>\n",
       "      <td>0.524482</td>\n",
       "      <td>0.717280</td>\n",
       "      <td>0.692145</td>\n",
       "      <td>0.758360</td>\n",
       "      <td>0.737606</td>\n",
       "      <td>0.744048</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.746220</td>\n",
       "      <td>0.735864</td>\n",
       "      <td>0.665801</td>\n",
       "      <td>0.748884</td>\n",
       "      <td>0.712587</td>\n",
       "      <td>0.756577</td>\n",
       "      <td>0.728072</td>\n",
       "      <td>0.709957</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.758369</td>\n",
       "      <td>0.745707</td>\n",
       "      <td>0.782693</td>\n",
       "      <td>0.766024</td>\n",
       "      <td>0.710621</td>\n",
       "      <td>0.775650</td>\n",
       "      <td>0.742924</td>\n",
       "      <td>0.703193</td>\n",
       "      <td>Naive_Bayes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.731039</td>\n",
       "      <td>0.715250</td>\n",
       "      <td>0.456005</td>\n",
       "      <td>0.720513</td>\n",
       "      <td>0.715655</td>\n",
       "      <td>0.736167</td>\n",
       "      <td>0.699595</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.740968</td>\n",
       "      <td>0.728251</td>\n",
       "      <td>0.575233</td>\n",
       "      <td>0.739887</td>\n",
       "      <td>0.712494</td>\n",
       "      <td>0.753904</td>\n",
       "      <td>0.719795</td>\n",
       "      <td>0.715978</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.751415</td>\n",
       "      <td>0.722910</td>\n",
       "      <td>0.660606</td>\n",
       "      <td>0.746320</td>\n",
       "      <td>0.707459</td>\n",
       "      <td>0.748818</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.709923</td>\n",
       "      <td>Logistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.763910</td>\n",
       "      <td>0.714642</td>\n",
       "      <td>0.787535</td>\n",
       "      <td>0.757440</td>\n",
       "      <td>0.717410</td>\n",
       "      <td>0.764310</td>\n",
       "      <td>0.746327</td>\n",
       "      <td>0.743048</td>\n",
       "      <td>Naive_Bayes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.736167</td>\n",
       "      <td>0.746964</td>\n",
       "      <td>0.435088</td>\n",
       "      <td>0.725641</td>\n",
       "      <td>0.684885</td>\n",
       "      <td>0.730904</td>\n",
       "      <td>0.710121</td>\n",
       "      <td>0.647368</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.745212</td>\n",
       "      <td>0.726335</td>\n",
       "      <td>0.559683</td>\n",
       "      <td>0.738983</td>\n",
       "      <td>0.698149</td>\n",
       "      <td>0.745402</td>\n",
       "      <td>0.724753</td>\n",
       "      <td>0.690031</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Processing_Type Performance_Metrics  Logistic       SVM  Naive_Bayes  \\\n",
       "0          standard            Accuracy  0.751415  0.730636     0.639927   \n",
       "1          standard           Precision  0.757197  0.737140     0.764118   \n",
       "2          standard              Recall  0.746694  0.720513     0.404588   \n",
       "3          standard            F1 score  0.749844  0.726904     0.524482   \n",
       "4    stopwords_nltk            Accuracy  0.746220  0.735864     0.665801   \n",
       "5    stopwords_nltk           Precision  0.758369  0.745707     0.782693   \n",
       "6    stopwords_nltk              Recall  0.731039  0.715250     0.456005   \n",
       "7    stopwords_nltk            F1 score  0.740968  0.728251     0.575233   \n",
       "8   stopwords_spacy            Accuracy  0.751415  0.722910     0.660606   \n",
       "9   stopwords_spacy           Precision  0.763910  0.714642     0.787535   \n",
       "10  stopwords_spacy              Recall  0.736167  0.746964     0.435088   \n",
       "11  stopwords_spacy            F1 score  0.745212  0.726335     0.559683   \n",
       "\n",
       "         LDA       QDA  Random_Forest   XGboost  Adaboost     Best_model  \n",
       "0   0.728139  0.710057       0.759207  0.740992  0.751315  Random_Forest  \n",
       "1   0.749784  0.732146       0.768149  0.747541  0.767877  Random_Forest  \n",
       "2   0.689474  0.658570       0.751552  0.735223  0.725101  Random_Forest  \n",
       "3   0.717280  0.692145       0.758360  0.737606  0.744048  Random_Forest  \n",
       "4   0.748884  0.712587       0.756577  0.728072  0.709957  Random_Forest  \n",
       "5   0.766024  0.710621       0.775650  0.742924  0.703193    Naive_Bayes  \n",
       "6   0.720513  0.715655       0.736167  0.699595  0.730769  Random_Forest  \n",
       "7   0.739887  0.712494       0.753904  0.719795  0.715978  Random_Forest  \n",
       "8   0.746320  0.707459       0.748818  0.730769  0.709923       Logistic  \n",
       "9   0.757440  0.717410       0.764310  0.746327  0.743048    Naive_Bayes  \n",
       "10  0.725641  0.684885       0.730904  0.710121  0.647368            SVM  \n",
       "11  0.738983  0.698149       0.745402  0.724753  0.690031  Random_Forest  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art5_perform_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processing_Type</th>\n",
       "      <th>Performance_Metrics</th>\n",
       "      <th>Logistic</th>\n",
       "      <th>SVM</th>\n",
       "      <th>Naive_Bayes</th>\n",
       "      <th>LDA</th>\n",
       "      <th>QDA</th>\n",
       "      <th>Random_Forest</th>\n",
       "      <th>XGboost</th>\n",
       "      <th>Adaboost</th>\n",
       "      <th>Best_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>standard</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.746187</td>\n",
       "      <td>0.722910</td>\n",
       "      <td>0.642557</td>\n",
       "      <td>0.733267</td>\n",
       "      <td>0.704729</td>\n",
       "      <td>0.759274</td>\n",
       "      <td>0.728039</td>\n",
       "      <td>0.707326</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>standard</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.759248</td>\n",
       "      <td>0.729040</td>\n",
       "      <td>0.829899</td>\n",
       "      <td>0.748143</td>\n",
       "      <td>0.693201</td>\n",
       "      <td>0.779113</td>\n",
       "      <td>0.750506</td>\n",
       "      <td>0.714789</td>\n",
       "      <td>Naive_Bayes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>standard</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.725506</td>\n",
       "      <td>0.710256</td>\n",
       "      <td>0.358165</td>\n",
       "      <td>0.710121</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.741161</td>\n",
       "      <td>0.689069</td>\n",
       "      <td>0.694062</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>standard</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.739231</td>\n",
       "      <td>0.718686</td>\n",
       "      <td>0.496535</td>\n",
       "      <td>0.726622</td>\n",
       "      <td>0.712235</td>\n",
       "      <td>0.756560</td>\n",
       "      <td>0.716435</td>\n",
       "      <td>0.702174</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.738428</td>\n",
       "      <td>0.743490</td>\n",
       "      <td>0.670929</td>\n",
       "      <td>0.735897</td>\n",
       "      <td>0.712554</td>\n",
       "      <td>0.748785</td>\n",
       "      <td>0.733233</td>\n",
       "      <td>0.712621</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.744629</td>\n",
       "      <td>0.743459</td>\n",
       "      <td>0.786377</td>\n",
       "      <td>0.738808</td>\n",
       "      <td>0.920287</td>\n",
       "      <td>0.774255</td>\n",
       "      <td>0.739292</td>\n",
       "      <td>0.748335</td>\n",
       "      <td>QDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.730904</td>\n",
       "      <td>0.746424</td>\n",
       "      <td>0.466397</td>\n",
       "      <td>0.736032</td>\n",
       "      <td>0.483806</td>\n",
       "      <td>0.715250</td>\n",
       "      <td>0.731039</td>\n",
       "      <td>0.647773</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.735979</td>\n",
       "      <td>0.743764</td>\n",
       "      <td>0.584885</td>\n",
       "      <td>0.735942</td>\n",
       "      <td>0.611498</td>\n",
       "      <td>0.741895</td>\n",
       "      <td>0.733783</td>\n",
       "      <td>0.693294</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.753946</td>\n",
       "      <td>0.728039</td>\n",
       "      <td>0.676124</td>\n",
       "      <td>0.743623</td>\n",
       "      <td>0.707326</td>\n",
       "      <td>0.746387</td>\n",
       "      <td>0.735864</td>\n",
       "      <td>0.712654</td>\n",
       "      <td>Logistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.760242</td>\n",
       "      <td>0.735012</td>\n",
       "      <td>0.786609</td>\n",
       "      <td>0.747532</td>\n",
       "      <td>0.899419</td>\n",
       "      <td>0.762202</td>\n",
       "      <td>0.734371</td>\n",
       "      <td>0.732999</td>\n",
       "      <td>QDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.751687</td>\n",
       "      <td>0.715385</td>\n",
       "      <td>0.482051</td>\n",
       "      <td>0.741161</td>\n",
       "      <td>0.488934</td>\n",
       "      <td>0.720783</td>\n",
       "      <td>0.741161</td>\n",
       "      <td>0.683806</td>\n",
       "      <td>Logistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.752285</td>\n",
       "      <td>0.722401</td>\n",
       "      <td>0.596669</td>\n",
       "      <td>0.741861</td>\n",
       "      <td>0.609430</td>\n",
       "      <td>0.739177</td>\n",
       "      <td>0.737416</td>\n",
       "      <td>0.703673</td>\n",
       "      <td>Logistic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Processing_Type Performance_Metrics  Logistic       SVM  Naive_Bayes  \\\n",
       "0          standard            Accuracy  0.746187  0.722910     0.642557   \n",
       "1          standard           Precision  0.759248  0.729040     0.829899   \n",
       "2          standard              Recall  0.725506  0.710256     0.358165   \n",
       "3          standard            F1 score  0.739231  0.718686     0.496535   \n",
       "4    stopwords_nltk            Accuracy  0.738428  0.743490     0.670929   \n",
       "5    stopwords_nltk           Precision  0.744629  0.743459     0.786377   \n",
       "6    stopwords_nltk              Recall  0.730904  0.746424     0.466397   \n",
       "7    stopwords_nltk            F1 score  0.735979  0.743764     0.584885   \n",
       "8   stopwords_spacy            Accuracy  0.753946  0.728039     0.676124   \n",
       "9   stopwords_spacy           Precision  0.760242  0.735012     0.786609   \n",
       "10  stopwords_spacy              Recall  0.751687  0.715385     0.482051   \n",
       "11  stopwords_spacy            F1 score  0.752285  0.722401     0.596669   \n",
       "\n",
       "         LDA       QDA  Random_Forest   XGboost  Adaboost     Best_model  \n",
       "0   0.733267  0.704729       0.759274  0.728039  0.707326  Random_Forest  \n",
       "1   0.748143  0.693201       0.779113  0.750506  0.714789    Naive_Bayes  \n",
       "2   0.710121  0.736842       0.741161  0.689069  0.694062  Random_Forest  \n",
       "3   0.726622  0.712235       0.756560  0.716435  0.702174  Random_Forest  \n",
       "4   0.735897  0.712554       0.748785  0.733233  0.712621  Random_Forest  \n",
       "5   0.738808  0.920287       0.774255  0.739292  0.748335            QDA  \n",
       "6   0.736032  0.483806       0.715250  0.731039  0.647773            SVM  \n",
       "7   0.735942  0.611498       0.741895  0.733783  0.693294            SVM  \n",
       "8   0.743623  0.707326       0.746387  0.735864  0.712654       Logistic  \n",
       "9   0.747532  0.899419       0.762202  0.734371  0.732999            QDA  \n",
       "10  0.741161  0.488934       0.720783  0.741161  0.683806       Logistic  \n",
       "11  0.741861  0.609430       0.739177  0.737416  0.703673       Logistic  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art5_perform_200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Performance of the best model for Article 5\n",
    "\n",
    "- Random Forest\n",
    "\n",
    "- 200 dimensional GloVe\n",
    "\n",
    "- No Stop words removed\n",
    "\n",
    "- Validation score: 0.759274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.670157</td>\n",
       "      <td>0.94958</td>\n",
       "      <td>0.664706</td>\n",
       "      <td>0.782007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall  F1_score\n",
       "0  0.670157    0.94958  0.664706  0.782007"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art5_test = test_performance('5', '200', 'standard', 'Random_Forest')\n",
    "art5_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading train set\n",
    "art6_train = pd.read_csv('./processed_text_df/train_article_6.csv')\n",
    "# labels\n",
    "train_y = art6_train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100d\n",
    "res_art6_100, art6_perform_100 = evaluate_performance_random(art6_train, train_y, GloVe_100, parameters, 5, scoring)\n",
    "pickle.dump(res_art6_100, open( \"./models/model_art6_100.p\", \"wb\" ))\n",
    "art6_perform_100.to_csv('./models/art6_100.csv')\n",
    "\n",
    "# 200d\n",
    "res_art6_200, art6_perform_200 = evaluate_performance_random(art6_train, train_y, GloVe_200, parameters, 5, scoring)\n",
    "pickle.dump(res_art6_200, open( \"./models/model_art6_200.p\", \"wb\" ))\n",
    "art6_perform_200.to_csv('./models/art6_200.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processing_Type</th>\n",
       "      <th>Performance_Metrics</th>\n",
       "      <th>Logistic</th>\n",
       "      <th>SVM</th>\n",
       "      <th>Naive_Bayes</th>\n",
       "      <th>LDA</th>\n",
       "      <th>QDA</th>\n",
       "      <th>Random_Forest</th>\n",
       "      <th>XGboost</th>\n",
       "      <th>Adaboost</th>\n",
       "      <th>Best_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>standard</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.772516</td>\n",
       "      <td>0.773405</td>\n",
       "      <td>0.625010</td>\n",
       "      <td>0.773379</td>\n",
       "      <td>0.760376</td>\n",
       "      <td>0.777734</td>\n",
       "      <td>0.771636</td>\n",
       "      <td>0.760380</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>standard</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.797353</td>\n",
       "      <td>0.799538</td>\n",
       "      <td>0.578568</td>\n",
       "      <td>0.797905</td>\n",
       "      <td>0.762321</td>\n",
       "      <td>0.798733</td>\n",
       "      <td>0.785245</td>\n",
       "      <td>0.779000</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>standard</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.730885</td>\n",
       "      <td>0.729100</td>\n",
       "      <td>0.927061</td>\n",
       "      <td>0.732564</td>\n",
       "      <td>0.756852</td>\n",
       "      <td>0.742984</td>\n",
       "      <td>0.748186</td>\n",
       "      <td>0.727331</td>\n",
       "      <td>Naive_Bayes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>standard</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.762298</td>\n",
       "      <td>0.712115</td>\n",
       "      <td>0.763614</td>\n",
       "      <td>0.759404</td>\n",
       "      <td>0.769580</td>\n",
       "      <td>0.766033</td>\n",
       "      <td>0.751923</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.760376</td>\n",
       "      <td>0.752569</td>\n",
       "      <td>0.684032</td>\n",
       "      <td>0.767333</td>\n",
       "      <td>0.764717</td>\n",
       "      <td>0.777730</td>\n",
       "      <td>0.776010</td>\n",
       "      <td>0.760429</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.775473</td>\n",
       "      <td>0.767201</td>\n",
       "      <td>0.642003</td>\n",
       "      <td>0.772198</td>\n",
       "      <td>0.762132</td>\n",
       "      <td>0.791425</td>\n",
       "      <td>0.786707</td>\n",
       "      <td>0.769360</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.732579</td>\n",
       "      <td>0.723973</td>\n",
       "      <td>0.838471</td>\n",
       "      <td>0.758561</td>\n",
       "      <td>0.770735</td>\n",
       "      <td>0.753373</td>\n",
       "      <td>0.758591</td>\n",
       "      <td>0.746462</td>\n",
       "      <td>Naive_Bayes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.752971</td>\n",
       "      <td>0.744643</td>\n",
       "      <td>0.726597</td>\n",
       "      <td>0.764847</td>\n",
       "      <td>0.765820</td>\n",
       "      <td>0.771506</td>\n",
       "      <td>0.771456</td>\n",
       "      <td>0.756818</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.764720</td>\n",
       "      <td>0.749972</td>\n",
       "      <td>0.683166</td>\n",
       "      <td>0.766452</td>\n",
       "      <td>0.762981</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.762127</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.781634</td>\n",
       "      <td>0.752595</td>\n",
       "      <td>0.640476</td>\n",
       "      <td>0.786523</td>\n",
       "      <td>0.770392</td>\n",
       "      <td>0.790947</td>\n",
       "      <td>0.785835</td>\n",
       "      <td>0.765830</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.734333</td>\n",
       "      <td>0.744678</td>\n",
       "      <td>0.843688</td>\n",
       "      <td>0.730855</td>\n",
       "      <td>0.749895</td>\n",
       "      <td>0.744678</td>\n",
       "      <td>0.753433</td>\n",
       "      <td>0.756867</td>\n",
       "      <td>Naive_Bayes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.756887</td>\n",
       "      <td>0.748020</td>\n",
       "      <td>0.727369</td>\n",
       "      <td>0.757246</td>\n",
       "      <td>0.759172</td>\n",
       "      <td>0.766744</td>\n",
       "      <td>0.768968</td>\n",
       "      <td>0.760723</td>\n",
       "      <td>XGboost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Processing_Type Performance_Metrics  Logistic       SVM  Naive_Bayes  \\\n",
       "0          standard            Accuracy  0.772516  0.773405     0.625010   \n",
       "1          standard           Precision  0.797353  0.799538     0.578568   \n",
       "2          standard              Recall  0.730885  0.729100     0.927061   \n",
       "3          standard            F1 score  0.762428  0.762298     0.712115   \n",
       "4    stopwords_nltk            Accuracy  0.760376  0.752569     0.684032   \n",
       "5    stopwords_nltk           Precision  0.775473  0.767201     0.642003   \n",
       "6    stopwords_nltk              Recall  0.732579  0.723973     0.838471   \n",
       "7    stopwords_nltk            F1 score  0.752971  0.744643     0.726597   \n",
       "8   stopwords_spacy            Accuracy  0.764720  0.749972     0.683166   \n",
       "9   stopwords_spacy           Precision  0.781634  0.752595     0.640476   \n",
       "10  stopwords_spacy              Recall  0.734333  0.744678     0.843688   \n",
       "11  stopwords_spacy            F1 score  0.756887  0.748020     0.727369   \n",
       "\n",
       "         LDA       QDA  Random_Forest   XGboost  Adaboost     Best_model  \n",
       "0   0.773379  0.760376       0.777734  0.771636  0.760380  Random_Forest  \n",
       "1   0.797905  0.762321       0.798733  0.785245  0.779000            SVM  \n",
       "2   0.732564  0.756852       0.742984  0.748186  0.727331    Naive_Bayes  \n",
       "3   0.763614  0.759404       0.769580  0.766033  0.751923  Random_Forest  \n",
       "4   0.767333  0.764717       0.777730  0.776010  0.760429  Random_Forest  \n",
       "5   0.772198  0.762132       0.791425  0.786707  0.769360  Random_Forest  \n",
       "6   0.758561  0.770735       0.753373  0.758591  0.746462    Naive_Bayes  \n",
       "7   0.764847  0.765820       0.771506  0.771456  0.756818  Random_Forest  \n",
       "8   0.766452  0.762981       0.774259  0.774259  0.762127  Random_Forest  \n",
       "9   0.786523  0.770392       0.790947  0.785835  0.765830  Random_Forest  \n",
       "10  0.730855  0.749895       0.744678  0.753433  0.756867    Naive_Bayes  \n",
       "11  0.757246  0.759172       0.766744  0.768968  0.760723        XGboost  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art6_perform_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processing_Type</th>\n",
       "      <th>Performance_Metrics</th>\n",
       "      <th>Logistic</th>\n",
       "      <th>SVM</th>\n",
       "      <th>Naive_Bayes</th>\n",
       "      <th>LDA</th>\n",
       "      <th>QDA</th>\n",
       "      <th>Random_Forest</th>\n",
       "      <th>XGboost</th>\n",
       "      <th>Adaboost</th>\n",
       "      <th>Best_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>standard</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.769034</td>\n",
       "      <td>0.764728</td>\n",
       "      <td>0.638890</td>\n",
       "      <td>0.781197</td>\n",
       "      <td>0.766452</td>\n",
       "      <td>0.782074</td>\n",
       "      <td>0.776010</td>\n",
       "      <td>0.769923</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>standard</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.793748</td>\n",
       "      <td>0.775866</td>\n",
       "      <td>0.590290</td>\n",
       "      <td>0.813713</td>\n",
       "      <td>0.800528</td>\n",
       "      <td>0.804704</td>\n",
       "      <td>0.799103</td>\n",
       "      <td>0.799594</td>\n",
       "      <td>LDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>standard</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.725652</td>\n",
       "      <td>0.746462</td>\n",
       "      <td>0.918381</td>\n",
       "      <td>0.729085</td>\n",
       "      <td>0.710015</td>\n",
       "      <td>0.744708</td>\n",
       "      <td>0.737781</td>\n",
       "      <td>0.720420</td>\n",
       "      <td>Naive_Bayes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>standard</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.757932</td>\n",
       "      <td>0.760394</td>\n",
       "      <td>0.717997</td>\n",
       "      <td>0.768867</td>\n",
       "      <td>0.751878</td>\n",
       "      <td>0.773234</td>\n",
       "      <td>0.766920</td>\n",
       "      <td>0.757626</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.770777</td>\n",
       "      <td>0.761250</td>\n",
       "      <td>0.696183</td>\n",
       "      <td>0.779469</td>\n",
       "      <td>0.765575</td>\n",
       "      <td>0.780327</td>\n",
       "      <td>0.776025</td>\n",
       "      <td>0.765598</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.789823</td>\n",
       "      <td>0.770561</td>\n",
       "      <td>0.655594</td>\n",
       "      <td>0.804660</td>\n",
       "      <td>0.829699</td>\n",
       "      <td>0.790489</td>\n",
       "      <td>0.794481</td>\n",
       "      <td>0.782224</td>\n",
       "      <td>QDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.736102</td>\n",
       "      <td>0.744708</td>\n",
       "      <td>0.833313</td>\n",
       "      <td>0.737781</td>\n",
       "      <td>0.668351</td>\n",
       "      <td>0.762084</td>\n",
       "      <td>0.746432</td>\n",
       "      <td>0.736042</td>\n",
       "      <td>Naive_Bayes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stopwords_nltk</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.761345</td>\n",
       "      <td>0.757036</td>\n",
       "      <td>0.733145</td>\n",
       "      <td>0.769344</td>\n",
       "      <td>0.739637</td>\n",
       "      <td>0.775670</td>\n",
       "      <td>0.768794</td>\n",
       "      <td>0.758288</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.766437</td>\n",
       "      <td>0.757779</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>0.778592</td>\n",
       "      <td>0.766433</td>\n",
       "      <td>0.776864</td>\n",
       "      <td>0.770792</td>\n",
       "      <td>0.754312</td>\n",
       "      <td>LDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.785184</td>\n",
       "      <td>0.758843</td>\n",
       "      <td>0.653882</td>\n",
       "      <td>0.798436</td>\n",
       "      <td>0.810876</td>\n",
       "      <td>0.792226</td>\n",
       "      <td>0.789842</td>\n",
       "      <td>0.768597</td>\n",
       "      <td>QDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.732624</td>\n",
       "      <td>0.755127</td>\n",
       "      <td>0.829835</td>\n",
       "      <td>0.744693</td>\n",
       "      <td>0.694333</td>\n",
       "      <td>0.749910</td>\n",
       "      <td>0.737811</td>\n",
       "      <td>0.727316</td>\n",
       "      <td>Naive_Bayes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stopwords_spacy</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.757708</td>\n",
       "      <td>0.756569</td>\n",
       "      <td>0.730672</td>\n",
       "      <td>0.770264</td>\n",
       "      <td>0.747750</td>\n",
       "      <td>0.770286</td>\n",
       "      <td>0.762897</td>\n",
       "      <td>0.746921</td>\n",
       "      <td>Random_Forest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Processing_Type Performance_Metrics  Logistic       SVM  Naive_Bayes  \\\n",
       "0          standard            Accuracy  0.769034  0.764728     0.638890   \n",
       "1          standard           Precision  0.793748  0.775866     0.590290   \n",
       "2          standard              Recall  0.725652  0.746462     0.918381   \n",
       "3          standard            F1 score  0.757932  0.760394     0.717997   \n",
       "4    stopwords_nltk            Accuracy  0.770777  0.761250     0.696183   \n",
       "5    stopwords_nltk           Precision  0.789823  0.770561     0.655594   \n",
       "6    stopwords_nltk              Recall  0.736102  0.744708     0.833313   \n",
       "7    stopwords_nltk            F1 score  0.761345  0.757036     0.733145   \n",
       "8   stopwords_spacy            Accuracy  0.766437  0.757779     0.693578   \n",
       "9   stopwords_spacy           Precision  0.785184  0.758843     0.653882   \n",
       "10  stopwords_spacy              Recall  0.732624  0.755127     0.829835   \n",
       "11  stopwords_spacy            F1 score  0.757708  0.756569     0.730672   \n",
       "\n",
       "         LDA       QDA  Random_Forest   XGboost  Adaboost     Best_model  \n",
       "0   0.781197  0.766452       0.782074  0.776010  0.769923  Random_Forest  \n",
       "1   0.813713  0.800528       0.804704  0.799103  0.799594            LDA  \n",
       "2   0.729085  0.710015       0.744708  0.737781  0.720420    Naive_Bayes  \n",
       "3   0.768867  0.751878       0.773234  0.766920  0.757626  Random_Forest  \n",
       "4   0.779469  0.765575       0.780327  0.776025  0.765598  Random_Forest  \n",
       "5   0.804660  0.829699       0.790489  0.794481  0.782224            QDA  \n",
       "6   0.737781  0.668351       0.762084  0.746432  0.736042    Naive_Bayes  \n",
       "7   0.769344  0.739637       0.775670  0.768794  0.758288  Random_Forest  \n",
       "8   0.778592  0.766433       0.776864  0.770792  0.754312            LDA  \n",
       "9   0.798436  0.810876       0.792226  0.789842  0.768597            QDA  \n",
       "10  0.744693  0.694333       0.749910  0.737811  0.727316    Naive_Bayes  \n",
       "11  0.770264  0.747750       0.770286  0.762897  0.746921  Random_Forest  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art6_perform_200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Performance of the best model for Article 5\n",
    "\n",
    "- Random Forest\n",
    "\n",
    "- 200 dimensional GloVe\n",
    "\n",
    "- No stop words removed\n",
    "\n",
    "- Validation score: 0.782074"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.746627</td>\n",
       "      <td>0.977974</td>\n",
       "      <td>0.736318</td>\n",
       "      <td>0.840114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall  F1_score\n",
       "0  0.746627   0.977974  0.736318  0.840114"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art6_test = test_performance('6', '200', 'standard', 'Random_Forest')\n",
    "art6_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating results \n",
    "perform = pd.concat([art2_test, art3_test, art5_test, art6_test]).reset_index(drop = True)\n",
    "perform = perform.set_index(pd.Index(['Article 2', 'Article 3', 'Article 5', 'Article 6']))\n",
    "perform['Best_model'] = ['Adaboost', 'Random Forest', 'Random Forest', 'Random Forest']\n",
    "perform = perform[['Best_model', 'Accuracy', 'Precision', 'Recall', 'F1_score' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best_model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Article 2</th>\n",
       "      <td>Adaboost</td>\n",
       "      <td>0.701493</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.811321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.730392</td>\n",
       "      <td>0.948905</td>\n",
       "      <td>0.730337</td>\n",
       "      <td>0.825397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.670157</td>\n",
       "      <td>0.949580</td>\n",
       "      <td>0.664706</td>\n",
       "      <td>0.782007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.746627</td>\n",
       "      <td>0.977974</td>\n",
       "      <td>0.736318</td>\n",
       "      <td>0.840114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Best_model  Accuracy  Precision    Recall  F1_score\n",
       "Article 2       Adaboost  0.701493   0.895833  0.741379  0.811321\n",
       "Article 3  Random Forest  0.730392   0.948905  0.730337  0.825397\n",
       "Article 5  Random Forest  0.670157   0.949580  0.664706  0.782007\n",
       "Article 6  Random Forest  0.746627   0.977974  0.736318  0.840114"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
