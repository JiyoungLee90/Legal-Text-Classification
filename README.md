{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Judicial Decisions of the European Court of Human Rights (ECHR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "***\n",
    "\n",
    "- Developed a classifier to predict whether a particular Article of the European Convention on Human Rights has been violated, given textual evidence extracted from a case.\n",
    "\n",
    "\n",
    "- Parsed and collected case information and judgement document for each ECHR case from HUDOC(Human Rights Documentation) API using Python.\n",
    "\n",
    "\n",
    "- Preprocessed judgement documents to generate text data and sorted them by the revelant Article. \n",
    "\n",
    "\n",
    "- Extracted features from the text data using pre-trained word embedding models.\n",
    "\n",
    "\n",
    "- Optimised and compared machine learning classification models built on different feature matrix by varing hyperparameters not only in machine learning models but also preprocessing methods, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code and Resources Used\n",
    "***\n",
    "**Packages:** pandas, numpy, sklearn, xgboost, selenium, json, os, sqlalchemy, psycopg2, nltk, scipy, gensim, requests, doxc, re, zipfile, ast \n",
    "\n",
    "**Legal Text Classification Framework**\n",
    "\n",
    "- Aletras N, Tsarapatsanis D, Preoţiuc-Pietro D, Lampos V (2016) Predicting judicial decisions of the European Court of Human Rights: a natural language processing perspective. PeerJ Comput Sci 2:e93.\n",
    "\n",
    "**HUDOC Data Collection** \n",
    "\n",
    "- HUDOC database : https://hudoc.echr.coe.int/eng#{\"documentcollectionid2\":[\"GRANDCHAMBER\",\"CHAMBER\"]}\n",
    "\n",
    "- Quemy, A. (2018), ‘European court of human right open data project’, arXiv preprint arXiv:1810.03115.\n",
    "\n",
    "**Word Embedding**\n",
    "\n",
    "- GloVe : https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "- Pennington, J., Socher, R., and Manning, C. D. (2014). Glove: Global vectors for word representation. Proceedings of the 2014 Conference on Empiricial Methods in Natural Language Processing (EMNLP-2014).\n",
    "\n",
    "- Law2Vec : https://archive.org/details/Law2Vec\n",
    "\n",
    "**Random Search**\n",
    "\n",
    "https://web.archive.org/web/20160701182750/http://blog.dato.com/how-to-evaluate-machine-learning-models-part-4-hyperparameter-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Generation Process\n",
    "***\n",
    "\n",
    "The datasets based on the raw information and documents available publicaly in HUDOC database.\n",
    "\n",
    "### 1. Retrieving case information from HUDOC database\n",
    "\n",
    "- Retriedved information about ECHR cases available from HUDOC API and saved in JSON files. \n",
    "\n",
    "- Filtered case information to keep only the cases in English, with an attached judgement documents(MS Word), and with a clear conclusion.\n",
    "\n",
    "- Cleaned case information by parsing and formatting so that it can be more easily accessible. \n",
    "\n",
    "- Sorted case information by Article with at least 100 associated cases.\n",
    "\n",
    "- Converted case information JSON files to Pandas DataFrames and stored in PostgreSQL database. \n",
    "\n",
    "### 2. Getting judgement documents from HUDOC database\n",
    "\n",
    "- Downloaded the judgement documents in MS Word format using HUDOC API and case id retrieved from case information.\n",
    "\n",
    "### 3. Parsing judgement documents \n",
    "\n",
    "- Parsed judgement documents to extract text from only relevant part of judgement documents by removing THE LAW and conclusion part. \n",
    "\n",
    "    - THE LAW part was removed since it includes arguments and discussions of judges that contain the information about the final case outcome. \n",
    "\n",
    "    - The conclusion part was removed in order to avoid data leakage as it contains information about the target variable (i.e, violation or no violation). \n",
    "\n",
    "\n",
    "### 4.  Generating dataset\n",
    "\n",
    "- Created dataset using the text data extracted from the previous step for each Article.  \n",
    "\n",
    "    - Each row represents a legal case from ECHR.\n",
    "    - The columns are case id(i.e, Itemid), judgement text, and judicial decision(Judgement).\n",
    "    - Any cases that were missing judgement text were marked as 'unavailable'.\n",
    "    \n",
    "    \n",
    "- Combined datasets from each Article into one dataframe to be stored in a PostgresSQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing \n",
    "***\n",
    "There are 3 variations of the text obtained from varying stop words treatement. \n",
    "Following text preprocessing steps were applied:\n",
    "\n",
    "- Lowercasing\n",
    "\n",
    "- Removing numbers, special characters & punctuations\n",
    "\n",
    "- Word tokenisation\n",
    "\n",
    "\n",
    "- Stop words \n",
    "\n",
    "    - No stopwords treatement; referred to 'standard'\n",
    "    - Remove stop words using NLTK stop words list\n",
    "    - Remove stop words using SpaCy stop words list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering \n",
    "***\n",
    "\n",
    "Pre-trained **GloVe** word embedding models were used to extract features from the text. A word embedding is a learned representation for text in an n-dimensional space where words that have the same meaning have a similar representation. In this project, this particular NLP method was employed in order to \n",
    "\n",
    "- transform textual data into numerical vectors.\n",
    "\n",
    "- capture the semantics of the words in the text dataset.\n",
    "\n",
    "Each judgement text was represented by the average of the word embedding vectors of the words that compose the text.\n",
    "\n",
    "The process of converting text to an average of word embedding vectors is as follows: \n",
    "\n",
    "For each preprocessed judgement text in the dataset, \n",
    "1. replaced each word by a vector representation of the word.\n",
    "\n",
    "2. averaged all these vectors to produce the vector representation of the judgement text from a case in the dataset. \n",
    "\n",
    "#### Dimensions of GloVe Word Embedding\n",
    "\n",
    "Both 100 and 200 dimension of GloVe word embeddings were considered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building & Evaluation\n",
    "***\n",
    "\n",
    "### Train-Test Split Method and Evaluation Metrics \n",
    "\n",
    "Correctly identifying both violation and no-violation cases are equally important, therefore, **accuracy** was selected to evaluate the model. However, it was found that there are far more violation cases than no-violation cases for each Article. With the high imbalance in the number of cases per class, many machine learning models are subject to a frequency bias in which they place more emphasis on learning from data observations which occur more commonly. Thus, the models are likely to just learn the distribution of class and simply predict the majority class for every unseen case. In such a case, accuracy do not adequately represent performance as the accuracy is only reflecting the underlying class distribution. \n",
    "\n",
    "In order to tackle this issue, undersampling was performed to reduce the number of violation cases in training set as follows:\n",
    "\n",
    "- Select 10% of no-violation cases for a test set and the rest for a training set. \n",
    "\n",
    "- Using the number of no-violation cases in the test set, choose the number of violation cases for the test set so that the ratio of each class in the original dataset is maintained in the test set. \n",
    "\n",
    "- Using the number of violation cases in the training set, choose the same number of violation cases for the training set for a balanced training set. \n",
    "\n",
    "### Machine Learning Classification Models\n",
    "\n",
    "The models that were considered:\n",
    "\n",
    "- Logistic Regression \n",
    "\n",
    "- SVM\n",
    "\n",
    "- Naive Bayes\n",
    "\n",
    "- Linear Discriminant Analysis\n",
    "\n",
    "- Qudratic Discrimant Analysis\n",
    "\n",
    "- Random Forest\n",
    "\n",
    "- Gradient Boosted Trees (XGBoost)\n",
    "\n",
    "- Ada Boost\n",
    "\n",
    "\n",
    "### Variations Resulting in Different Feature Matrices\n",
    "\n",
    "Following variations were considered\n",
    "\n",
    "- preprocessing the text (3 versions of judgement text)\n",
    "\n",
    "- dimensions of GloVe word embedding (2 versions)\n",
    "\n",
    "for experimentation. These variations affected the learning process by resulting in a different feature matrix. Every combination of these variations were used and compared to find the optimal model just like finding the best parameters in grid search. \n",
    "\n",
    "\n",
    "### Hyperparameter Tuning with Random Search \n",
    "\n",
    "Random search was performed to find the optimal set of hyperparameters as it seems more feasible method when there are many hyperparameters to tune. \n",
    "\n",
    "For a given Article, each version of text, and each dimensional word vectors, following steps were applied to find the optimal model: \n",
    "\n",
    "- For each classification models, \n",
    "\n",
    "    1. convert the text data into an average vector using a customised vectoriser. \n",
    "\n",
    "    2. scale the feature matrix using Min-Max scaler to normalise them.\n",
    "\n",
    "    3. select random combinations from a grid of hyperparameter values and train the model using 5-fold cross-validation. Find the optimal set of paramter which gives the highest accuracy. \n",
    "\n",
    "    4. refit the model with the best found parameters on the whole training set and store the parameter values and scores. \n",
    "    \n",
    "\n",
    "- Compare the performance of models and find the best model.\n",
    "\n",
    "    - i.e, Article 2 model performance table \n",
    "    \n",
    "        - 100-dimensional\n",
    "        \n",
    "         <img src=\"resources/art2_100.png\">\n",
    "        \n",
    "        - 200-dimensional\n",
    "        \n",
    "        <img src=\"resources/art2_200.png\">\n",
    "\n",
    "\n",
    "- Make predictions on test set using the best model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance\n",
    "***\n",
    "\n",
    "<img src=\"resources/performance.png\" align = \"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
