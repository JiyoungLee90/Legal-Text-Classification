{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating dataset\n",
    "\n",
    "A dataframe for each article is created. In each dataframe, \n",
    "\n",
    "- Each row represents a legal case from ECHR. \n",
    "\n",
    "- The columns are case id(i.e, Itemid), judgement text, and judicial decision(Judgement).  \n",
    "\n",
    "While creating the dataframe, following cleaning steps were made:\n",
    "\n",
    "1. any cases that were missing judgement text were marked as 'unavailable'.\n",
    "\n",
    "2. dataframes were shuffled as violation and no-violation cases were merged to form the dataframe for each article, it created the structure of all rows from violation cases are on top, followed by the rows with no-violation. \n",
    "\n",
    "Dataframes were combined into one dataframe to be stored in a PostgresSQL database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, listdir, mkdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe for each article \n",
    "def article_df(input_folder, article_number):\n",
    "    ''' Collect data for textual analysis and make dataframe for each article \n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    input_folder : str\n",
    "                   path for where judgement documents per article is stored\n",
    "    article_number : str\n",
    "                     article number to retrieve the judgement documents \n",
    "    '''\n",
    "    \n",
    "    art = 'article_{}'.format(article_number)\n",
    "    folder = path.join(input_folder, art)\n",
    "    judgement = ['violation', 'no-violation']\n",
    "    df_list = []\n",
    "    \n",
    "    for j in judgement:\n",
    "        df = pd.read_csv(path.join(folder, 'case_outcome.csv'))\n",
    "        id_list = list(df[df.Judgement == j].Itemid) # get case id list from case outcome df\n",
    "        \n",
    "        doc_list = listdir(path.join(folder, j)) # get list of availble text docs in the folder\n",
    "        doc_name_list = [x.replace('_Judgement_text.txt', '') for x in doc_list]\n",
    "    \n",
    "        # find missing cases  \n",
    "        id_list.sort()\n",
    "        doc_name_list.sort()\n",
    "        doc_path = []\n",
    "        text_list = []\n",
    "        if id_list == doc_name_list: \n",
    "            for i in id_list:\n",
    "                text_path = '{}_Judgement_text.txt'.format(i)\n",
    "                doc_path.append(text_path)\n",
    "                # loading the text\n",
    "                with open(path.join(folder, j, text_path), 'r') as t:\n",
    "                    text = t.read()\n",
    "                    text_list.append(text)\n",
    "                    t.close()\n",
    "            # check empty textfile\n",
    "            if '' in text_list:\n",
    "                empty_count = text_list.count('')\n",
    "                text_list = ['unavailable' if doc == '' else doc for doc in text_list]\n",
    "                print(\"{} missing {} documents in {}\".format(empty_count ,j, art))\n",
    "            else:\n",
    "                print (\"No missing {} documents in {}\".format(j, art)) \n",
    "                \n",
    "        # missing docs            \n",
    "        else: \n",
    "            diff_len = len(id_list) - len(doc_name_list)\n",
    "            missing_id = np.setdiff1d(id_list, doc_name_list)\n",
    "            for i in id_list:\n",
    "                text_path = '{}_Judgement_text.txt'.format(i)\n",
    "                if i in missing_id:\n",
    "                    doc_path.append('unavailable')\n",
    "                    text_list.append('unavailable')\n",
    "                else:\n",
    "                    doc_path.append(text_path)\n",
    "                    with open(path.join(folder, j, text_path), 'r') as t:\n",
    "                        text = t.read()\n",
    "                        text_list.append(text)\n",
    "                        t.close()\n",
    "            # check empty textfile\n",
    "            if '' in text_list:\n",
    "                empty_count = text_list.count('')\n",
    "                text_list = ['unavailable' if doc == '' else doc for doc in text_list]\n",
    "                print (\"{} missing {} documents in {}\".format(diff_len + empty_count, j, art))\n",
    "            else:\n",
    "                print (\"{} missing {} documents in {}\".format(diff_len, j, art))\n",
    "                \n",
    "        data_frame = pd.DataFrame(list(zip(id_list, doc_path, text_list)), \n",
    "                                  columns = ['Itemid', 'Document_path', 'Judgement_text'])\n",
    "        data_frame['Judgement'] = j\n",
    "        df_list.append(data_frame)\n",
    "\n",
    "    article_df = pd.concat([df_list[0], df_list[1]], ignore_index = True)\n",
    "    \n",
    "    return article_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = './HUDOC_data/docs_per_article'\n",
    "article_number = ['2', '3', '5', '6', '8', '10', '11', '13', '14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 missing violation documents in article_2\n",
      "1 missing no-violation documents in article_2\n",
      "9 missing violation documents in article_3\n",
      "3 missing no-violation documents in article_3\n",
      "8 missing violation documents in article_5\n",
      "No missing no-violation documents in article_5\n",
      "32 missing violation documents in article_6\n",
      "7 missing no-violation documents in article_6\n",
      "8 missing violation documents in article_8\n",
      "4 missing no-violation documents in article_8\n",
      "3 missing violation documents in article_10\n",
      "No missing no-violation documents in article_10\n",
      "1 missing violation documents in article_11\n",
      "No missing no-violation documents in article_11\n",
      "13 missing violation documents in article_13\n",
      "2 missing no-violation documents in article_13\n",
      "4 missing violation documents in article_14\n",
      "3 missing no-violation documents in article_14\n"
     ]
    }
   ],
   "source": [
    "article_df_list = []\n",
    "for a in article_number:\n",
    "    df = article_df(input_folder, a)\n",
    "    article_df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle rows of each df\n",
    "article_df_list = [df.sample(frac = 1).reset_index(drop = True) for df in article_df_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir('./articles')\n",
    "for a in range(0, 9):\n",
    "    # saving dataframe for each article as csv file\n",
    "    article_df_list[a].to_csv('articles/article_{}'.format(article_number[a]), index = False)\n",
    "    # provide article label to indicate which dataframe the entries come from\n",
    "    article_df_list[a]['Article'] = article_number[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the dataframe \n",
    "concat_df = pd.concat(article_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['case_info_14', 'case_info_10', 'case_info_p1-1', 'case_info_6', 'case_info_13', 'case_info_5', 'case_info_3', 'case_info_8', 'case_info_34', 'case_info_2', 'case_info_11', 'case_info', 'judgement_text']\n"
     ]
    }
   ],
   "source": [
    "# store it into PostgresSQL \n",
    "engine = create_engine(\"postgresql://postgres:xfkLVeMj@localhost/hudoc\")\n",
    "con = engine.connect()\n",
    "table_name = 'judgement_text'\n",
    "concat_df.to_sql(table_name, con)\n",
    "print(engine.table_names())\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
