{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case information collection\n",
    "\n",
    "\n",
    "### Process:\n",
    "\n",
    "1. parse case information from HUDOC API\n",
    "\n",
    "2. filter case information \n",
    "\n",
    "3. clean case information\n",
    "\n",
    "4. sort case information by article\n",
    "\n",
    "5. store in PostgreSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from time import sleep\n",
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from psycopg2.extras import Json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Retrieving the case information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''pulls maximum number of documents from HUDOC.'''\n",
    "\n",
    "# use safari as a webdriver \n",
    "driver = webdriver.Safari()\n",
    "\n",
    "# get the number of available docs in HUDOC \n",
    "url = \"https://hudoc.echr.coe.int/eng#%20\"\n",
    "driver.implicitly_wait(10)\n",
    "driver.get(url)\n",
    "result = driver.find_element_by_class_name('resultNumber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172819"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## text to int\n",
    "max_docs = int(result.text)\n",
    "max_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''pulls case information from HUDOC API.'''\n",
    "\n",
    "# Fields to retrieve \n",
    "fields = [\n",
    "    \"sharepointid\", \n",
    "    \"Rank\", \n",
    "    \"itemid\", \n",
    "    \"docname\", \n",
    "    \"doctype\", \n",
    "    \"application\", \n",
    "    \"appno\", \n",
    "    \"conclusion\", \n",
    "    \"importance\", \n",
    "    \"originatingbody\", \n",
    "    \"typedescription\", \n",
    "    \"kpdate\", \n",
    "    \"kpdateAsText\", \n",
    "    \"documentcollectionid\", \n",
    "    \"documentcollectionid2\", \n",
    "    \"languageisocode\", \n",
    "    \"extractedappno\", \n",
    "    \"isplaceholder\", \n",
    "    \"doctypebranch\", \n",
    "    \"respondent\", \n",
    "    \"respondentOrderEng\", \n",
    "    \"ecli\", \n",
    "    \"article\", \n",
    "    \"applicability\", \n",
    "    \"decisiondate\", \n",
    "    \"externalsources\", \n",
    "    \"introductiondate\", \n",
    "    \"issue\", \n",
    "    \"judgementdate\", \n",
    "    \"kpthesaurus\", \n",
    "    \"meetingnumber\", \n",
    "    \"representedby\", \n",
    "    \"separateopinion\", \n",
    "    \"scl\"  \n",
    "]\n",
    "\n",
    "base_url = \"http://hudoc.echr.coe.int/app/query/results?query=((((((((((((((((((((%20contentsitename%3AECHR%20AND%20(NOT%20(doctype%3DPR%20OR%20doctype%3DHFCOMOLD%20OR%20doctype%3DHECOMOLD)))%20XRANK(cb%3D14)%20doctypebranch%3AGRANDCHAMBER)%20XRANK(cb%3D13)%20doctypebranch%3ADECGRANDCHAMBER)%20XRANK(cb%3D12)%20doctypebranch%3ACHAMBER)%20XRANK(cb%3D11)%20doctypebranch%3AADMISSIBILITY)%20XRANK(cb%3D10)%20doctypebranch%3ACOMMITTEE)%20XRANK(cb%3D9)%20doctypebranch%3AADMISSIBILITYCOM)%20XRANK(cb%3D8)%20doctypebranch%3ADECCOMMISSION)%20XRANK(cb%3D7)%20doctypebranch%3ACOMMUNICATEDCASES)%20XRANK(cb%3D6)%20doctypebranch%3ACLIN)%20XRANK(cb%3D5)%20doctypebranch%3AADVISORYOPINIONS)%20XRANK(cb%3D4)%20doctypebranch%3AREPORTS)%20XRANK(cb%3D3)%20doctypebranch%3AEXECUTION)%20XRANK(cb%3D2)%20doctypebranch%3AMERITS)%20XRANK(cb%3D1)%20doctypebranch%3ASCREENINGPANEL)%20XRANK(cb%3D4)%20importance%3A1)%20XRANK(cb%3D3)%20importance%3A2)%20XRANK(cb%3D2)%20importance%3A3)%20XRANK(cb%3D1)%20importance%3A4)%20XRANK(cb%3D2)%20languageisocode%3AENG)%20XRANK(cb%3D1)%20languageisocode%3AFRE&select={}&sort=&rankingModelId=4180000c-8692-45ca-ad63-74bc4163871b\".format(','.join(fields))\n",
    "length = 500 ## max items per request\n",
    "\n",
    "# make a directory to save the request output\n",
    "output_folder = './raw_cases_info'\n",
    "if os.path.exists(output_folder):\n",
    "    shutil.rmtree(output_folder)\n",
    "os.mkdir(output_folder)\n",
    "\n",
    "# request case information from the HUDOC api\n",
    "## tracking of failed cases\n",
    "number_failed = 0 \n",
    "output_folder = './raw_cases_info'\n",
    "\n",
    "for start in range(0, max_docs, length):\n",
    "    with open(os.path.join(output_folder, \"%d.json\"%(start)), 'wb') as f:\n",
    "        url = base_url + \"&start=%d&length=%d\"%(start, length)\n",
    "        res = requests.get(url, stream = True)\n",
    "        if not res.ok:\n",
    "            print('failed to fetch information {} to {}'.format(start, start + length))\n",
    "            number_failed += 1\n",
    "            continue\n",
    "        for chunk in res.iter_content(chunk_size = 1024):\n",
    "            f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Filtering the case information\n",
    "For this project, we keep only cases\n",
    "\n",
    "- in English\n",
    "- with an attatched judgement document\n",
    "- with a clear conclusion (clearly stating \"violation\" or \"no violation\" at least once) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all case info \n",
    "cases = []\n",
    "input_folder = './raw_cases_info'\n",
    "## get all the directories\n",
    "files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if os.path.isfile(os.path.join(input_folder, f)) if '.json' in f]\n",
    "for i in files:\n",
    "    with open(i, 'r') as f:\n",
    "        content = f.read()\n",
    "        index = json.loads(content)\n",
    "        cases.extend(index['results'])\n",
    "cases = [c['columns'] for c in cases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171831"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter cases\n",
    "total = len(cases)\n",
    "total\n",
    "## remove non-english cases\n",
    "cases = [c for c in cases if c[\"languageisocode\"] == \"ENG\"]\n",
    "## remove cases with no judgement document\n",
    "cases = [c for c in cases if c[\"doctype\"] == \"HEJUD\"]\n",
    "## remove cases without an attached judgement document\n",
    "cases = [c for c in cases if c[\"application\"].startswith(\"MS WORD\")]\n",
    "## remove cases without a clear conclusion\n",
    "cases = [c for c in cases if \"No-violation\" in c[\"conclusion\"] or \"No violation\" in c[\"conclusion\"] or \"Violation\" in c[\"conclusion\"] or \"violation\" in c[\"conclusion\"]]\n",
    "## remove a specific list of cases hard to process\n",
    "cases = [c for c in cases if c['itemid'] not in [\"001-154354\", \"001-108395\", \"001-79411\"]]\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cleaning the case information\n",
    "\n",
    "- parse and format some raw information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parties(docname):\n",
    "    ''' Parse the case title(docname) and return the list of parties.\n",
    "    \n",
    "    parameter\n",
    "    ---------\n",
    "    docname : str \n",
    "              string containing the parties name \n",
    "    '''\n",
    "    if docname.startswith('CASE OF '):\n",
    "        docname = docname[len('CASE OF '):]\n",
    "    if docname[-1] == ')':\n",
    "        docname = docname.split('(')[0]\n",
    "    parties = docname.split(' v.')\n",
    "    parties = [p.strip() for p in parties]\n",
    "    \n",
    "    return parties\n",
    "\n",
    "def split_article(article):\n",
    "    ''' Parse string containing the list of articles into \n",
    "        the list of articles. \n",
    "    \n",
    "    parameter\n",
    "    ---------\n",
    "    article : str \n",
    "              string containing the list of articles \n",
    "    '''\n",
    "\n",
    "    parts = article.split('+')\n",
    "    articles = [parts[-1]]\n",
    "    for k, e in enumerate(parts[:-1]):\n",
    "        if not parts[k + 1].startswith(e):\n",
    "            articles.append(e)\n",
    "    return articles\n",
    "\n",
    "def base_article(articles):\n",
    "    ''' Find the base articles from a list of articles \n",
    "    \n",
    "    parameter\n",
    "    ---------\n",
    "    articles : str \n",
    "               list of articles \n",
    "    '''\n",
    "    base_articles = []\n",
    "    for a in articles:\n",
    "        a = a.split('+')[0]\n",
    "        if 'p' not in a.lower():\n",
    "            base_articles.append(a.split('-', 1)[0])\n",
    "        else:\n",
    "            base_articles.append('-'.join(a.split('-')[0:2]))\n",
    "\n",
    "    return base_articles\n",
    "\n",
    "def article(article):\n",
    "    ''' Format the list of articles.\n",
    "\n",
    "    parameter\n",
    "    ---------\n",
    "    article : str\n",
    "              string containing the list of articles\n",
    "\n",
    "    '''\n",
    "    articles = article.lower().split(';')\n",
    "    return list(set(base_article(\n",
    "        [item for sublist in list(map(split_article, articles)) for item in sublist])))\n",
    "\n",
    "\n",
    "def subarticle(article):\n",
    "    ''' Format the list of subarticles.\n",
    "\n",
    "    parameter\n",
    "    ---------\n",
    "    article : str\n",
    "              string containing the list of articles\n",
    "\n",
    "    '''\n",
    "    articles = article.split(';')\n",
    "    articles = [a for sublist in articles for a in sublist.split('+')]\n",
    "    res = list(set(articles))\n",
    "    return res\n",
    "\n",
    "def merge_conclusion_elements(elements):\n",
    "\n",
    "    ''' Merge similar conclusion elements in a single one, more descriptive\n",
    "    \n",
    "    parameter\n",
    "    ----------\n",
    "    elements : dict\n",
    "               conclusion elements\n",
    "    '''\n",
    "\n",
    "    final_elements = {}\n",
    "    for e in elements:\n",
    "        if 'article' in e and 'base_article' in e:\n",
    "            key = '{}_{}_{}'.format(e['article'], e['base_article'], e['element'])\n",
    "        else:\n",
    "            key = e['element']\n",
    "        if key not in final_elements:\n",
    "            final_elements[key] = e\n",
    "        final_elements[key].update(e)\n",
    "    return list(final_elements.values())\n",
    "\n",
    "def get_element_type(l):\n",
    "    t = 'other'\n",
    "    if l.startswith('violation'):\n",
    "        t = 'violation'\n",
    "    elif l.startswith('no-violation') or l.startswith('no violation'):\n",
    "        t = 'no-violation'\n",
    "    return t\n",
    "\n",
    "def format_conclusion_elements(i, e, final_ccl):\n",
    "    to_append = []\n",
    "    l = e['element'].lower().strip()\n",
    "\n",
    "    # Determine type\n",
    "    t = get_element_type(l)\n",
    "    final_ccl[i]['type'] = t\n",
    "    if t == 'other':\n",
    "        to_append.append(final_ccl[i])\n",
    "\n",
    "    # Determine articles\n",
    "    articles = []\n",
    "    if 'protocol' in e['element'].lower():\n",
    "        prot = e['element'].lower().split('protocol no.')\n",
    "        f1 = prot[0].split()[-2]\n",
    "        f2 = prot[1].split()[0]\n",
    "        final_ccl[i]['article'] = f'p{f2}-{f1}'\n",
    "        articles = split_article(final_ccl[i]['article'])\n",
    "\n",
    "    if 'article' not in final_ccl[i] and t != 'other':\n",
    "        art = None\n",
    "        find_and_replace = [\n",
    "            (' and art. ', ''),\n",
    "            (' and of ', '+'),\n",
    "            (' and ', '+')\n",
    "        ]\n",
    "        for p in find_and_replace:\n",
    "            if p[0] in l:\n",
    "                l = l.replace(p[0], p[1])\n",
    "\n",
    "        b = l.split()\n",
    "        for j, a in enumerate(b):\n",
    "            if a.startswith('art'):\n",
    "                if a.lower().startswith('art.') and not a.lower().startswith('art. ') and len(a) > 4:\n",
    "                    art = a.lower()[4:]\n",
    "                else:\n",
    "                    art = b[j + 1]\n",
    "                break\n",
    "        if art is not None:\n",
    "            articles = split_article(art)\n",
    "            art = art.split('+')\n",
    "            if '+' in art[0]:\n",
    "                sart = art[0].split('+')\n",
    "                t = [sart[-1]]\n",
    "                for k, e in enumerate(sart[:-1]):\n",
    "                    if not sart[k + 1].startswith(e):\n",
    "                        t.append(e)\n",
    "\n",
    "    base_articles = base_article(articles)\n",
    "    for k, art in enumerate(articles):\n",
    "        item = copy.copy(final_ccl[i])\n",
    "        item['article'] = art\n",
    "        item['base_article'] = base_articles[k]\n",
    "        to_append.append(item)\n",
    "\n",
    "    return to_append\n",
    "\n",
    "\n",
    "def conclusion(ccl):\n",
    "    ''' Convert a conclusion string into a list of elements\n",
    "    \n",
    "    parameter\n",
    "    ---------\n",
    "    ccl : str\n",
    "          conclusion string \n",
    "    \n",
    "    '''\n",
    "    final_ccl = []\n",
    "    chunks = [c for c in ccl.split(')') if len(c)]\n",
    "    art = []\n",
    "    for c in chunks:\n",
    "        if '(' not in c:\n",
    "            art.extend(c.split(';'))\n",
    "        else:\n",
    "            art.append(c)\n",
    "    art = [a for a in art if len(a) > 0]\n",
    "    for c in art:\n",
    "        a = c.split('(')\n",
    "        b = a[1].split(';') if len(a) > 1 else None\n",
    "        articles = [d.strip() for d in a[0].split(';')]\n",
    "        articles = [d for d in articles if len(d) > 0]\n",
    "        if not len(articles):\n",
    "            if b:\n",
    "                if 'mentions' in final_ccl[-1]:\n",
    "                    final_ccl[-1]['mentions'].extend(b)\n",
    "                else:\n",
    "                    final_ccl[-1]['mentions'] = b\n",
    "            continue\n",
    "        article = articles[-1] if not articles[-1].startswith(';') else articles[-1][1:]\n",
    "        conclusion = {'element': article }\n",
    "        if b:\n",
    "            conclusion['details'] = b\n",
    "        if len(article.strip()) == 0:\n",
    "            if b is not None:\n",
    "                final_ccl[-1]['mentions'] = b\n",
    "        else:\n",
    "            final_ccl.append(conclusion)\n",
    "    if len(articles) > 1:\n",
    "        for a in articles[:-1]:\n",
    "            if len(a) > 0:\n",
    "                final_ccl.append({'element': a})\n",
    "\n",
    "    to_append = []\n",
    "    for i, e in enumerate(final_ccl):\n",
    "        to_append.extend(format_conclusion_elements(i, e, final_ccl))\n",
    "\n",
    "    final_ccl = merge_conclusion_elements(to_append)\n",
    "    \n",
    "    return final_ccl\n",
    "\n",
    "# apply parsing and formatting\n",
    "for i, c in enumerate(cases):\n",
    "        cases[i]['parties'] = parties(cases[i]['docname'])\n",
    "        cases[i]['conclusion_detail'] = cases[i]['conclusion']\n",
    "        cases[i]['conclusion'] = conclusion(c['conclusion_detail'])\n",
    "        cases[i]['articles_detail'] = cases[i]['article']\n",
    "        cases[i]['article'] = article(cases[i]['articles_detail'])\n",
    "        cases[i]['paragraphs'] = subarticle(cases[i]['articles_detail'])\n",
    "        cases[i]['externalsources'] = cases[i][\"externalsources\"].split(';') if len(\n",
    "            cases[i]['externalsources']) > 0 else []\n",
    "        cases[i][\"documentcollectionid\"] = cases[i][\"documentcollectionid\"].split(';') if len(\n",
    "            cases[i]['documentcollectionid']) > 0 else []\n",
    "        cases[i][\"issue\"] = cases[i][\"issue\"].split(';') if len(cases[i]['issue']) > 0 else []\n",
    "        cases[i][\"representedby\"] = cases[i][\"representedby\"].split(';') if len(\n",
    "            cases[i]['representedby']) > 0 else []\n",
    "        cases[i][\"extractedappno\"] = cases[i][\"extractedappno\"].split(';')\n",
    "\n",
    "        cases[i]['externalsources'] = [e.strip() for e in cases[i]['externalsources']]\n",
    "        cases[i]['documentcollectionid'] = [e.strip() for e in cases[i]['documentcollectionid']]\n",
    "        cases[i]['issue'] = [e.strip() for e in cases[i]['issue']]\n",
    "        cases[i]['representedby'] = [e.strip() for e in cases[i]['representedby']]\n",
    "        cases[i]['extractedappno'] = [e.strip() for e in cases[i]['extractedappno']]\n",
    "\n",
    "all_cases = cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sorting the case information by article\n",
    "\n",
    "- sort case information by each unique article with at least 100 associated cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 10: 669 \n",
      "Article p1-1: 1363 \n",
      "Article 14: 432 \n",
      "Article 6: 7340 \n",
      "Article 13: 1865 \n",
      "Article 5: 2370 \n",
      "Article 3: 2330 \n",
      "Article 9: 96 (removed)\n",
      "Article 8: 1375 \n",
      "Article 18: 36 (removed)\n",
      "Article 34: 169 \n",
      "Article 2: 809 \n",
      "Article p1-3: 64 (removed)\n",
      "Article p12-1: 5 (removed)\n",
      "Article 11: 274 \n",
      "Article p7-4: 31 (removed)\n",
      "Article p1-2: 27 (removed)\n",
      "Article p4-2: 49 (removed)\n",
      "Article 25: 7 (removed)\n",
      "Article 38: 41 (removed)\n",
      "Article 7: 88 (removed)\n",
      "Article p7-2: 14 (removed)\n",
      "Article 4: 17 (removed)\n",
      "Article 12: 19 (removed)\n",
      "Article p7-3: 1 (removed)\n",
      "Article p4-4: 9 (removed)\n",
      "Article p7-1: 5 (removed)\n",
      "Article 46: 1 (removed)\n",
      "Article p6-1: 4 (removed)\n",
      "Article .5: 1 (removed)\n",
      "Article 17: 1 (removed)\n"
     ]
    }
   ],
   "source": [
    "# clean cases before sorting\n",
    "cleaned_cases = []\n",
    "for c in cases:\n",
    "    labels = []\n",
    "    for i in c['conclusion']:\n",
    "        if i['type'] in ['violation', 'no-violation']:\n",
    "            if 'article' in i:\n",
    "                j = i['article']\n",
    "                labels.append('{}:{}'.format(j, 1 if i['type'] == 'violation' else 0))\n",
    "    labels = list(set(labels)) \n",
    "    # double checking if wrong assignment of label\n",
    "    opposed_labels = any([i for i in labels if i.split(':')[0] + ':'+ str(abs(1-int(i.split(':')[-1])))in labels])\n",
    "    if len(labels) > 0 and not opposed_labels:\n",
    "        cleaned_cases.append(c)\n",
    "\n",
    "# count number of cases per article\n",
    "outcomes = {}\n",
    "cases_per_articles = {}\n",
    "## iterate through the cleaned cases and count number of judgement outcome\n",
    "for i, c in enumerate(cleaned_cases):\n",
    "    ccl = c['conclusion']\n",
    "    for d in ccl:\n",
    "        if d['type'] in ['violation', 'no-violation']:\n",
    "            if d['base_article'] not in outcomes:\n",
    "                ### dictionary for counting\n",
    "                outcomes[d['base_article']] = {\n",
    "                    'violation' : 0,\n",
    "                    'no-violation' : 0,\n",
    "                    'total' : 0\n",
    "                }\n",
    "            outcomes[d['base_article']][d['type']] += 1\n",
    "            outcomes[d['base_article']]['total'] += 1\n",
    "            ### sort cases by articles  \n",
    "            if d['base_article'] not in cases_per_articles:\n",
    "                cases_per_articles[d['base_article']] = []\n",
    "            cases_per_articles[d['base_article']].append(c)\n",
    "\n",
    "# filter articles with less than 100\n",
    "for k, v in outcomes.items():\n",
    "    print('Article {}: {} {}'.format(k, v['total'], '(removed)' if v['total'] < 100 else ''))\n",
    "outcomes = {k:v for k,v in outcomes.items() if v['total'] > 100}\n",
    "\n",
    "# generate case info for specific articles\n",
    "df_lst = []\n",
    "for k in outcomes.keys():\n",
    "    ## store it in dataframe\n",
    "    df = pd.DataFrame(cases_per_articles[k])\n",
    "    ### drop duplicated cases\n",
    "    df.drop_duplicates(subset = ['itemid'], inplace = True)\n",
    "    df_lst.append(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export dataframe for each article as a csv\n",
    "## make a folder to save\n",
    "os.mkdir('./case_info')\n",
    "## article list\n",
    "art_lst = list(outcomes.keys())\n",
    "for i in range(len(df_lst)):\n",
    "    df_lst[i].to_csv('case_info/case_info_{}.csv'.format(art_lst[i]), index = False)\n",
    "    df_lst[i]['conclusion'] = df_lst[i]['conclusion'].astype(str) # as postgresql cannot accept 'dict' type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe for all cases before spliting into specific articles \n",
    "df = pd.DataFrame(all_cases)\n",
    "df.to_csv('case_info/case_info.csv', index = False)\n",
    "df.conclusion = df.conclusion.astype(str) # as postgresql cannot accept 'dict' type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Storing in PostgreSQL database \n",
    "\n",
    "- store case information data frame for each article \n",
    "- store all articles data frame \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to database \n",
    "engine = create_engine(\"postgresql://postgres:xfkLVeMj@localhost/hudoc\")\n",
    "con = engine.connect()\n",
    "\n",
    "# create table for all cases \n",
    "table_name = 'case_info'\n",
    "df.to_sql(table_name, con)\n",
    "print(engine.table_names())\n",
    "\n",
    "# create table for each articles\n",
    "table_name_lst = ['case_info_{}'.format(a) for a in art_lst]\n",
    "for i in range(len(df_lst)):\n",
    "    df_lst[i].to_sql(table_name_lst[i], con)   \n",
    "print(engine.table_names())\n",
    "\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
