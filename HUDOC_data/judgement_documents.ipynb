{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Judgement Text collection\n",
    "\n",
    "\n",
    "### Process:\n",
    "\n",
    "1. Download judgement doucuments from HUDOC API\n",
    "\n",
    "2. Preprocess judgement documents \n",
    "\n",
    "3. Sort texts by Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os import listdir, mkdir\n",
    "from os.path import isfile, join\n",
    "import requests\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "from docx.text.run import Run\n",
    "import zipfile\n",
    "import re\n",
    "import shutil\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Downloading documents\n",
    "\n",
    "- get corresponding judgement documents from HUDOC API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get case id from case information \n",
    "case_info = pd.read_csv('./case_info/case_info.csv')\n",
    "case_id_lst = list(case_info.itemid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = './raw_documents' # folder to save raw docs\n",
    "os.mkdir(output_folder)\n",
    "failed_case = [] # failed requests\n",
    "update == False\n",
    "# urls \n",
    "base_url = \"http://hudoc.echr.coe.int/app/conversion/docx/?library=ECHR&filename=please_give_me_the_document.docx&id=\"\n",
    "perm_url = \"http://hudoc.echr.coe.int/eng?i=\"\n",
    "\n",
    "# get documents\n",
    "for i, case_id in enumerate(case_id_lst):\n",
    "    ## tracking the process \n",
    "    print(\"Document {}/{}: {}\".format(i, len(case_id_lst), case_id))\n",
    "    filename = \"%s.docx\"%(case_id.strip())\n",
    "    filepath = join(output_folder, filename)\n",
    "    ## download the file if no such a file exists or update == True\n",
    "    if update or not isfile(filepath):\n",
    "        url = base_url + case_id.strip()\n",
    "        res = requests.get(url, stream = True)\n",
    "        if not res.ok:\n",
    "            print(\"Failed to fetch document %s\"%(case_id))\n",
    "            failed_case.append(case_id)\n",
    "            print(\"URL: %s\"%(url))\n",
    "            print(\"Permalink: %s\"%(perm_url + case_id.strip()))\n",
    "            continue\n",
    "        with open(filepath, 'wb') as f:\n",
    "            for block in res.iter_content(1024):\n",
    "                f.write(block)\n",
    "            print(\"Request complete, see %s\"%(filepath))\n",
    "    else:\n",
    "        print(\"Skip as document exists already\")           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing the documents \n",
    "\n",
    "- parsing the MS Word document to extract judgement text without the the entire text of the court decision :\n",
    "1. Conclusion - the text includes information about labels i.e, 'violation' or 'no violation'\n",
    "2. Law -  the text includes arguments and discussions of judges that partly contain the final decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP = '/tmp/echr_tmp_doc'\n",
    "\n",
    "# possible tags for each type of section\n",
    "tags = {\n",
    "    \"SECTION_TITLE_STYLE\": ['ECHR_Title_1', 'Ju_H_Head'],\n",
    "    \"HEADING_1_STYLE\": ['ECHR_Heading_1', 'Ju_H_I_Roman'],\n",
    "    \"HEADING_2_STYLE\" :['ECHR_Heading_2', 'Ju_H_A', 'Ju_H_a'],\n",
    "    \"HEADING_3_STYLE\" :['ECHR_Heading_3', 'Ju_H_1.', 'Ju_H_1'],\n",
    "    \"HEADING_PARA\": ['ECHR_Para', 'ECHR_Para_Quote', 'Ju_List',\\\n",
    "        'Ju_List_a', 'Ju_Para', 'Normal', 'Ju_Quot', 'Ju_H_Article',\\\n",
    "        'Ju_Para Char Char', 'Ju_Para Char', 'Ju_Para_Last', 'Opi_Para'],\n",
    "    \"DECISION_BODY\": ['ECHR_Decision_Body', 'Ju_Judges']\n",
    "}\n",
    "\n",
    "# different level of document to parse\n",
    "levels = {\n",
    "    \"DECISION_BODY\": -1,\n",
    "    \"SECTION_TITLE_STYLE\": 1,\n",
    "    \"HEADING_1_STYLE\": 2,\n",
    "    \"HEADING_2_STYLE\": 3,\n",
    "    \"HEADING_3_STYLE\": 4,\n",
    "    \"HEADING_PARA\": 5\n",
    "}\n",
    "\n",
    "tag_to_level = {}\n",
    "for k, v in tags.items():\n",
    "    for t in v:\n",
    "        tag_to_level[t] = levels[k]\n",
    "\n",
    "OLD_PARSER_TAGS = ['header', 'Normal', 'Body Text 2', 'Body Text Indent 3', 'OldCommission', 'Heading 6', 'Heading 5', 'Heading 4']\n",
    "\n",
    "internal_section_reference = {\n",
    "    'toc': [\"Table of contents\"],\n",
    "    'abbreviations': [\"ABBREVIATIONS AND ACRONYMS\"],\n",
    "    'introduction': [\"INTRODUCTION\"],\n",
    "    'procedure': [\"CLAIMS MADE BY THE APPLICANTS\", \"I.  Locus standı\", \"PROCEDURE\", \"PROCEDURE”\", \"AS TO PROCEDURE\",\n",
    "                  \"PROCEDURE AND FACTS\", \"FACTS AND PROCEDURE\", \"I.   THE GOVERNMENT’S PRELIMINARY OBJECTION\"],\n",
    "    'facts': [\"THE FACTS\", \"AS TO THE FACTS\", \"COMPLAINTS\", \"COMPLAINT\", \"FACTS\",\n",
    "              \"THE FACT\", \"THE FACTSITMarkFactsComplaintsStart\"\n",
    "              \"THE CIRCUMSTANCES OF THE CASE\",\n",
    "              \"I.  THE CIRCUMSTANCES OF THE CASE\",\n",
    "              \"I. THE PARTICULAR CIRCUMSTANCES OF THE CASE\"\n",
    "              'PROCEEDINGS', \"PROCEEDINGS BEFORE THE COMMISSION\",\n",
    "              \"II. PROCEEDINGS BEFORE THE COMMISSION\",\n",
    "              \"PROCEEDINGS BEFORE THE COMMISSION  17.\"\n",
    "              ],\n",
    "    'law': [\"THE LAW\",\n",
    "            \"LAW\",\n",
    "            \"IV.  COMPLIANCE WITH THE EXHAUSTION RULE\",\n",
    "            \"THE LAWS ON THE USE OF LANGUAGES IN EDUCATION IN\",\n",
    "            \"AS TO THE LAW\",\n",
    "            \"TO THE LAW\",\n",
    "            \"III. THE LAW\",\n",
    "            \"IN LAW\",\n",
    "            \"APPLICATION OF ARTICLE\",\n",
    "            \"II.  APPLICATION OF ARTICLE\",\n",
    "            \"IV.  COMPLIANCE WITH THE EXHAUSTION RULE\",\n",
    "            \"IV.  OTHER COMPLAINTS UNDER ARTICLE\",\n",
    "            \"I. ALLEGED LACK OF STANDING AS\",\n",
    "            \"ITMarkFactsComplaintsEndTHE LAW\",\n",
    "            \"ALLEGED VIOLATION OF ARTICLE\",\n",
    "            \"AS TO THE  ALLEGED VIOLATION OF ARTICLE\",\n",
    "            \"I.  ALLEGED VIOLATION OF ARTICLE\",\n",
    "            \"III.  ALLEGED VIOLATION OF ARTICLE\",\n",
    "            \"THE ALLEGED BREACHES OF ARTICLE\",\n",
    "            \"II.   ALLEGED VIOLATION OF ARTICLE\"\n",
    "            \"MERITS\", \"II.  MERITS\", \"III.  MERITS\"\n",
    "            ],\n",
    "    'conclusion': [\"CONCLUSION\",\n",
    "                   \"THE COURT UNANIMOUSLY\",\n",
    "                   \"REASONS, THE COURT, UNANIMOUSLY,\",\n",
    "                   \"FOR THESE REASONS, THE COURT UNANIMOUSLY\",\n",
    "                   \"FOR THESE REASONS, THE COURT ,UNANIMOUSLY,\",\n",
    "                   \"FOR THESE REASONS, THE COURT, UNANIMOUSLY,\",\n",
    "                   \"FOR THESE REASONS, THE COURT UNANIMOUSLY,\",\n",
    "                   \"FOR THESE REASONS, THE COURT,UNANIMOUSLY,\",\n",
    "                   \"FOR THESE REASONS, THE COURT, UNANIMOUSLY\",\n",
    "                   \"FOR THESE REASONS THE COURT UNANIMOUSLY\",\n",
    "                   \"FOR THESE REASONS, THE COURT UNANIMOUSLY:\",\n",
    "                   \"FOR THESE REASONS, THE COUR, UNANIMOUSLY,\",\n",
    "                   \"FOR THESE REASONS THE COURT\",\n",
    "                   \"FOR THESE RASONS, THE COURT UNANIMOUSLY\",\n",
    "                   \"FOR THESE REASONS, THE COURT:\",\n",
    "                   \"FOR THE REASONS, THE COURT\",\n",
    "                   \"THE COURT\",\n",
    "                   \"FOR THESE REASONS, THE COURT,\",\n",
    "                   \"FOR THESE REASONS, THE COURT\"],\n",
    "    'relevant_law': [\"RELEVANT DOMESTIC LAW\",\n",
    "                     \"II.  RELEVANT DOMESTIC LAW\",\n",
    "                     \"RELEVANT DOMESTIC LEGAL FRAMEWORK\",\n",
    "                     \"III.  RELEVANT ELEMENTS OF COMPARATIVE LAW\",\n",
    "                     \"II. RELEVANT DOMESTIC LAW\",\n",
    "                     \"II. RELEVANT DOMESTIC LAW AND PRACTICE\",\n",
    "                     \"RELEVANT DOMESTIC LAW AND CASE-LAW\",\n",
    "                     \"III.  RELEVANT INTERNATIONAL MATERIALS\",\n",
    "                     \"RELEVANT international material\",\n",
    "                     \"II.  RELEVANT DOMESTIC LAW AND PRACTICE\",\n",
    "                     \"RELEVANT DOMESTIC AND INTERNATIONAL LAW\",\n",
    "                     \"III.  RELEVANT INTERNATIONAL MATERIAL\",\n",
    "                     \"II.  RELEVANT DOMESTIC LAW AND PRACTICE AND INTERNATIONAL MATERIALS\"\n",
    "                     \"RELEVANT DOMESTIC LAW AND PRACTICE\",\n",
    "                     \"RELEVANT EUROPEAN UNION LAW\",\n",
    "                     'relevant legal framework',\n",
    "                     \"RELEVANT LEGAL FRAMEWORK AND PRACTICE\",\n",
    "                     \"III.  COMPARATIVE LAW AND PRACTICE\",\n",
    "                     \"RELEVANT LEGAL FRAMEWORK AND INTERNATIONAL MATERIAL\",\n",
    "                     \"RELEVANT LEGAL and factual FRAMEWORK\",\n",
    "                     \"RELEVANT LEGAL FRAMEWORK and the council of europe material\",\n",
    "                     \"Council of europe material\",\n",
    "                     \"LEGAL FRAMEWORK\",\n",
    "                     \"III.  RELEVANT INTERNATIONAL LAW\",\n",
    "                     \"RELEVANT COUNCIL OF EUROPE DOCUMENTS\",\n",
    "                     \"III.  RELEVANT COUNCIL OF EUROPE INSTRUMENTS\",\n",
    "                     \"II.  RELEVANT INTERNATIONAL MATERIAL\"],\n",
    "    \"opinion\": [\"STATEMENT OF DISSENT BY JUDGE KŪRIS\",\n",
    "                \"JOINT CONCURRING OPINION OF JUDGES YUDKIVSKA, VUČINIĆ, TURKOVIĆ AND HÜSEYNOV\",\n",
    "                \"JOINT PARTLY DISSENTING OPINION OF JUDGES RAIMONDI, SICILIANOS, KARAKAS, VUČINIĆ AND HARUTYUNYAN\",\n",
    "                \"PARTLY DISSENTING OPINION OF JUDGE DE GAETANO, JOINED BY JUDGE VUČINIĆ\",\n",
    "                \"PARTLY DISSENTING OPINION OF JUDGE KŪRIS\",\n",
    "                \"PARTLY DISSENTING OPINION OF JUDGE GROZEV\",\n",
    "                \"DISSENTING OPINION OF JUDGE KOSKELO\",\n",
    "                \"CONCURRING OPINION OF JUDGE PINTO DE ALBUQUERQUE\",\n",
    "                \"DISSENTING OPINION OF JUDGE BAKA\",\n",
    "                \"PARTLY DISSENTING OPINION OF JUDGE SICILIANOS\",\n",
    "                \"PARTLY DISSENTING OPINION OF JUDGE EICKE\",\n",
    "                \"PARTLY DISSENTING OPINION OF JUDGE EICKE\",\n",
    "                \"CONCURRING OPINION OF JUDGE JEBENS\",\n",
    "                \"CONCURRING OPINION OF JUDGE GÖLCÜKLÜ\",\n",
    "                \"ConcurRing opinion of Judge Bonello\",\n",
    "                \"CONCURRING OPINION OF JUDGE SERGHIDES\",\n",
    "                \"DISSENTING OPINION OF JUDGE SERGHIDES\",\n",
    "                \"DISSENTING OPINION OF JUDGE ROZAKIS\",\n",
    "                \"PARTLY DISSENTING OPINION OF JUDGE GÖLCÜKLÜ\",\n",
    "                \"JOINT DISSENTING OPINION OF JUDGES GROZEV AND O’LEARY\",\n",
    "                \"JOINT PARTLY DISSENTING OPINION OF JUDGES LOUCAIDES AND TULKENS\"],\n",
    "    \"appendix\": ['APPENDIX', \"APPENDIX: LIST OF APPLICANTS\", \"APPENDIX 1\", \"ANNEX\",\n",
    "                 \"APPENDIX 2\", \"ANNEX 1:\", \"ANNEX 2:\", \"Annex I\", \"Annex II\", \"Appendix to the judgment\"],\n",
    "    \"submission\": [\"FINAL SUBMISSIONS TO THE COURT\",\n",
    "                   \"THE GOVERNMENT’S FINAL SUBMISSIONS TO THE COURT\",\n",
    "                   \"FINAL SUBMISSIONS BY THE GOVERNMENT TO THE COURT\",\n",
    "                   \"FINAL SUBMISSIONS SUBMITTED TO THE COURT BY THE GOVERNMERNT\",\n",
    "                   \"DISSENTING OPINION OF JUDGE SCHEMBRI ORLAND\",\n",
    "                   \"GOVERNMENT’S FINAL SUBMISSIONS TO THE COURT\",\n",
    "                   \"FINAL SUBMISSIONS TO THE COURT BY THE GOVERNMENT\",\n",
    "                   \"FINAL SUBMISSIONS MADE TO THE COURT\",\n",
    "                   \"FOR THESE REASONS, THE COUR\",\n",
    "                   \"SUBMISSIONS OF THE PARTIES\",\n",
    "                   \"CONCLUDING SUBMISSIONS MADE TO THE COURT\",\n",
    "                   \"CONCLUDING SUBMISSIONS MADE TO THE COURT\",\n",
    "                   \"THE GOVERNMENT’S SUBMISSIONS TO THE COURT\",\n",
    "                   \"THE GOVERNMENT’S FINAL SUBMISSIONS\",\n",
    "                   \"FINAL SUBMISSIONS PRESENTED BY THE GOVERNMENT\",\n",
    "                   \"FINAL SUBMISSIONS PRESENTED TO THE COURT\",\n",
    "                   \"FINAL SUBMISSIONS AND OBSERVATIONS MADE TO THE COURT\",\n",
    "                   \"FINAL SUBMISSIONS AND OBSERVATIONS MADE TO THE COURT\",\n",
    "                   \"FINAL SUBMISSIONS MADE TO THE COURT BY THE GOVERNMENT\",\n",
    "                   \"FINAL SUBMISSIONS MADE BY THE GOVERNMENT TO THE COURT\",\n",
    "                   \"SUBMISSIONS MADE BY THE GOVERNMENT TO THE COURT\",\n",
    "                   \"CONCLUDING SUBMISSIONS BY THE GOVERNMENT\",\n",
    "                   \"FINAL SUBMISSIONS MADE BY THE GOVERNMENT\",\n",
    "                   \"FINAL SUBMISSIONS BY THOSE APPEARING BEFORE THE COURT\"],\n",
    "    'schedule': [\"SCHEDULE\"]\n",
    "}\n",
    "\n",
    "\n",
    "def tag_elements(parsed):\n",
    "    \"\"\"\n",
    "        Tag the elements in the parsed document.\n",
    "        Tag the elements in the parsed documents\n",
    "        according to some predifined sections.\n",
    "        :param parsed: parsed document\n",
    "        :type parsed: dict\n",
    "        :return: parsed document with internal section references\n",
    "        :rtype: dict\n",
    "    \"\"\"\n",
    "    for i, section in enumerate(parsed['elements']):\n",
    "        for section_reference, values in internal_section_reference.items():\n",
    "            if any(section['content'].strip().upper().startswith(v.upper()) for v in values):\n",
    "                parsed['elements'][i]['section_name'] = section_reference\n",
    "                break\n",
    "        #if not 'section_name' in parsed['elements'][i]:\n",
    "        #    print('Could not tag section {}'.format(section['content']))\n",
    "        #    print(section['content'])\n",
    "    return parsed\n",
    "\n",
    "\n",
    "def format_title(line):\n",
    "    \"\"\"Format title\n",
    "        :param line: line to format as title\n",
    "        :type line: str\n",
    "        :return: formatted title\n",
    "        :rtype: str\n",
    "    \"\"\"\n",
    "    m = re.match(r'(\\w+)\\.(.+)', line)\n",
    "    if m:\n",
    "        return m.group(2).strip()\n",
    "    else:\n",
    "        return line\n",
    "\n",
    "def parse_body(body):\n",
    "    \"\"\"Extract body members\n",
    "        :param body: line to extract the body members from\n",
    "        :type body: str\n",
    "        :return: list of members with their role\n",
    "        :rtype: [dict]\n",
    "    \"\"\"\n",
    "    members = []\n",
    "    body = body.replace('\\nand ', '\\n')\n",
    "    body = body.replace('\\t', '')\n",
    "    body = body.split('\\n')\n",
    "    body = [b for b in body if len(b)]\n",
    "\n",
    "    roles = []\n",
    "    k = 0\n",
    "    for i, t in enumerate(body):\n",
    "        a = [j for j in t.split(',') if len(j)]\n",
    "        members.append({'name': a[0]})\n",
    "        if len(a) > 1:\n",
    "            roles.append((k, i, a[1].lower().strip()))\n",
    "            k = i + 1\n",
    "\n",
    "    for r in roles:\n",
    "        for i, m in enumerate(members[r[0]:r[1]+1]):\n",
    "            members[r[0] + i]['role'] = r[2]\n",
    "\n",
    "    return members\n",
    "\n",
    "\n",
    "class Node:\n",
    "    \"\"\"Represent a rooted tree\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, parent=None, level=0, content=None):\n",
    "        self.parent = parent\n",
    "        self.level = level\n",
    "        self.content = content\n",
    "        self.elements = []\n",
    "\n",
    "\n",
    "def parse_document(doc):\n",
    "    \"\"\"Parse a document object to a tree\n",
    "        :param doc: document object\n",
    "        :type doc: Document\n",
    "        :return: tree\n",
    "        :rtype: Node\n",
    "    \"\"\"\n",
    "    parsed = {}\n",
    "\n",
    "    decision_body = \"\"\n",
    "    appender = Node() # Top level node\n",
    "    for p in doc.paragraphs:\n",
    "        line = p.text.strip()\n",
    "        if not len(line):\n",
    "            continue\n",
    "        #print(p.style.name, p.text)\n",
    "        level = tag_to_level.get(p.style.name, 0)\n",
    "        if level > 0:\n",
    "            if appender.level == 0 and not len(appender.elements) and level > 1:\n",
    "                pass\n",
    "                #print('HEADER')\n",
    "            else:\n",
    "                #print('L {} | App level: {}'.format(level, appender.level))\n",
    "                if level < appender.level:\n",
    "                    while(appender.level > level - 1):\n",
    "                        appender = appender.parent\n",
    "                elif level == appender.level:\n",
    "                    appender = appender.parent\n",
    "                node = Node(parent=appender, level=level, content=p.text)\n",
    "                appender.elements.append(node)\n",
    "                appender = node\n",
    "\n",
    "        if level < 0:\n",
    "            if level == -1:\n",
    "                #print(p.text)\n",
    "                decision_body += p.text\n",
    "        #else:\n",
    "        #    print(p.style.name, p.text)\n",
    "    \n",
    "\n",
    "    root = appender\n",
    "    while(root.level != 0):\n",
    "        root = root.parent\n",
    "\n",
    "    def print_tree(root):\n",
    "        \"\"\"Utilitary function to print tree\n",
    "            :param root: root of the tree\n",
    "            :type root: Node\n",
    "        \"\"\"\n",
    "        print(\"LEVEL {} {} {}\".format(root.level, ' ' * root.level * 2, root.content.encode('utf-8') if root.content else 'ROOT'))\n",
    "        if len(root.elements) == 0:\n",
    "            return\n",
    "        else:\n",
    "            for e in root.elements:\n",
    "                print_tree(e)\n",
    "\n",
    "    def tree_to_json(root, res):\n",
    "        \"\"\"Recursively convert a tree into json\n",
    "            :param root: root of the tree\n",
    "            :type root: Node\n",
    "            :param res: where to store result\n",
    "            :type: res: dict\n",
    "            :return: remaining tree\n",
    "            :rtype: Node\n",
    "        \"\"\"\n",
    "        node = {\n",
    "            'content': root.content,\n",
    "            'elements': []\n",
    "        }\n",
    "        for e in root.elements:\n",
    "            node['elements'].append(tree_to_json(e, node))\n",
    "        return node\n",
    "\n",
    "    parsed = {'elements': []}\n",
    "    parsed['elements'] = tree_to_json(root, parsed)['elements']\n",
    "    parsed['decision_body'] = parse_body(decision_body) if decision_body else []\n",
    "    parsed = tag_elements(parsed)\n",
    "\n",
    "    return parsed\n",
    "\n",
    "PARSER = {\n",
    "    'old': 'OLD',\n",
    "    'new': 'NEW'\n",
    "}\n",
    "\n",
    "def format_paragraph(p):\n",
    "    \"\"\"Format paragraph\n",
    "        :param line: line to format as title\n",
    "        :type line: str\n",
    "        :return: formatted title\n",
    "        :rtype: str\n",
    "    \"\"\"\n",
    "    match = re.search(r'^(?:\\w+\\.)(.+)', p)\n",
    "    if match is not None:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return p\n",
    "    \n",
    "def json_to_text_(doc, text_only=True, except_section=[]):\n",
    "    res = []\n",
    "    if not len(doc['elements']):\n",
    "        res.append(format_paragraph(doc['content']))\n",
    "    # text_only: remove the titles\n",
    "    for e in doc['elements']:\n",
    "        if not 'section_name' in e or e['section_name'] not in except_section:\n",
    "            res.extend(json_to_text_(e, text_only=True, except_section=except_section))\n",
    "    return res\n",
    "\n",
    "def json_to_text(doc, text_only=True, except_section=[]):\n",
    "    \"\"\"Format json to text \n",
    "        :param doc: parsed document\n",
    "        :type doc: dict\n",
    "        :param text_only: return only text\n",
    "        :type text_only: bool\n",
    "        :param except_section: list of section to discard\n",
    "        :type: except_section: list\n",
    "        :return: textual representation of the document\n",
    "        :rtype: str\n",
    "    \"\"\"\n",
    "    except_section = [] if except_section is None else except_section\n",
    "    return '\\n'.join(json_to_text_(doc, text_only, except_section))\n",
    "\n",
    "def select_parser(doc):\n",
    "    \"\"\"Select the parser to be used for a given document\n",
    "        :param doc: document\n",
    "        :type doc: Document\n",
    "        :return: parser name\n",
    "        :rtype: str\n",
    "    \"\"\"\n",
    "    if all([True if p.style.name in OLD_PARSER_TAGS else False for p in doc.paragraphs]):\n",
    "        return PARSER['old']\n",
    "    else:\n",
    "        return PARSER['new']\n",
    "    \n",
    "def update_docx(docname):\n",
    "    \"\"\"Update a docx such that it can be read by docx library.\n",
    "        MSWord documents are a zip folder containing several XML files.\n",
    "        As docx library cannot read 'smartTag', it is required to remove them.\n",
    "        To do so, we open the zip, access the main XML file and manually sanitize it.\n",
    "        :param docname: path to the document\n",
    "        :type docname: str\n",
    "        :return: path to the new document\n",
    "        :rtype: str\n",
    "    \"\"\"\n",
    "    # Remove temporary folder and files\n",
    "    try:\n",
    "        shutil.rmtree(TMP)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        os.rm('./_proxy.docx')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Extract the document\n",
    "    zip_ref = zipfile.ZipFile(docname, 'r')\n",
    "    zip_ref.extractall(TMP)\n",
    "    zip_ref.close()\n",
    "\n",
    "    # Sanitize\n",
    "    with open(os.path.join(TMP, 'word/document.xml'), 'r') as file:\n",
    "        content = file.read()\n",
    "        lines = content.split('>')\n",
    "        remove_open = True\n",
    "        for i, l in enumerate(lines):\n",
    "            if '<w:smartTag ' in l and remove_open:\n",
    "                del lines[i]\n",
    "                remove_open = False\n",
    "            if '</w:smartTag'==l and not remove_open:\n",
    "                del lines[i]\n",
    "                remove_open = True\n",
    "        file.close()\n",
    "    content = '>'.join(lines)\n",
    "\n",
    "    # Recompress the archive\n",
    "    with open(os.path.join(TMP, 'word/document.xml'), 'w') as file:\n",
    "        file.write(content)\n",
    "    shutil.make_archive('./proxy', 'zip', TMP)\n",
    "\n",
    "    output_file = './_proxy.docx'\n",
    "    os.rename('./proxy.zip', output_file)\n",
    "\n",
    "    try:\n",
    "        os.rm('./_proxy.docx')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return output_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = './raw_documents'\n",
    "mkdir(output_folder)\n",
    "output_folder = './preprocessed_documents'\n",
    "case_id_lst = list(case_info.itemid)\n",
    "\n",
    "# stat of parser type used\n",
    "stats = {\n",
    "    'parser_type':{\n",
    "        'OLD': 0,\n",
    "        'NEW': 0\n",
    "    }\n",
    "}\n",
    "\n",
    "update = False\n",
    "correctly_parsed = 0\n",
    "failed = []\n",
    "files = [join(input_folder, f) for f in listdir(input_folder) if isfile(join(input_folder, f)) if '.docx' in f]\n",
    "# process documents\n",
    "for i, f in enumerate(files):\n",
    "    id_doc = f.split('/')[-1].split('.')[0]\n",
    "    print('Process document {} {}/{}'.format(id_doc, i, len(files)))\n",
    "    filename_parsed = os.path.join(output_folder, '{}_Judgement_text.txt'.format(id_doc))\n",
    "    if update or not os.path.isfile(filename_parsed):\n",
    "        try:\n",
    "            f_updated = update_docx(f)\n",
    "            doc = Document(f_updated)\n",
    "            parser = select_parser(doc)\n",
    "            stats['parser_type'][parser] +=1\n",
    "            if parser == 'NEW':\n",
    "                parsed = parse_document(doc)\n",
    "                with open(os.path.join(output_folder, '{}_Judgement_text.txt'.format(id_doc)), 'wb') as toutfile:\n",
    "                    toutfile.write(json_to_text(parsed, True, ['law','conclusion']).encode('utf-8'))\n",
    "                    parsed['documents'] = ['{}.docx'.format(id_doc)]\n",
    "                    parsed['content'] = {\n",
    "                        '{}.docx'.format(id_doc): parsed['elements']\n",
    "                        }\n",
    "                    del parsed['elements']\n",
    "                    correctly_parsed += 1\n",
    "            else:\n",
    "                raise Exception(\"OLD parser is not available yet.\")\n",
    "        except Exception as e:\n",
    "            failed.append((id_doc, e))    \n",
    "            print(\"{} {}\".format(f, e))\n",
    "    else:\n",
    "        print('Skip document because it is already processed')\n",
    "        correctly_parsed += 1\n",
    "\n",
    "    print('Correctly parsed: {}/{} ({}%)'.format(correctly_parsed, len(files), (100. * correctly_parsed) / len(files)))\n",
    "    print('List of failed documents:')\n",
    "    \n",
    "    for e in failed:\n",
    "        print('{}: {}'.format(e[0], e[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3. Sorting the texts by Article \n",
    " \n",
    " - allocate parsed texts to corresponding outcome and Article folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dup(duplicate):\n",
    "    final_list = []\n",
    "    for item in duplicate:\n",
    "        if item not in final_list:\n",
    "            final_list.append(item)\n",
    "    return final_list\n",
    "\n",
    "\n",
    "def sort_docs(doc_folder, violation_folder, violation):\n",
    "    files = listdir(doc_folder)\n",
    "    for item_id in violation:\n",
    "        item_path = '{}_Judgement_text.txt'.format(item_id)\n",
    "        file_path = join(doc_folder, item_path)\n",
    "        if item_path in files:\n",
    "            output_path = join(violation_folder, item_path)\n",
    "            shutil.copy(file_path, violation_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = ('./case_info')\n",
    "doc_folder = './preprocessed_documents'\n",
    "output_folder = './docs_per_article'\n",
    "\n",
    "# check if there's directory\n",
    "#if path.isdir(output_folder):\n",
    "    #shutil.rmtree(output_folder)\n",
    "    #mkdir(output_folder)\n",
    "#else: \n",
    "mkdir(output_folder)\n",
    "    \n",
    "# get article list from file names stored in case_info folder\n",
    "files = [join(input_folder, f) for f in listdir(input_folder) if isfile(join(input_folder, f)) if '.csv' in f]\n",
    "article_lst = [f for f in files if f.find('info_') > 0]  \n",
    "\n",
    "for a in article_lst:\n",
    "    article = a[22:-4]\n",
    "    output_path = join(output_folder, 'article_{}'.format(article))\n",
    "    mkdir(output_path)\n",
    "    ### iterate through rows in conclusion column and find the case outcome\n",
    "    violation = []\n",
    "    no_violation = []\n",
    "    df = pd.read_csv(a) ### read df for specific article \n",
    "    df.conclusion = df.conclusion.apply(lambda x: literal_eval(x)) # convert it to the original list\n",
    "    for i in range(len(df)): ## iterate through each element in conclusion list \n",
    "        for c in df.conclusion[i]:\n",
    "            if 'base_article' in c:\n",
    "                art_num = c['base_article']\n",
    "            if art_num == article and c['type'] == 'violation':\n",
    "                violation.append(df.itemid[i])\n",
    "                violation = remove_dup(violation) ### remove duplicated cases\n",
    "            elif art_num == article and c['type'] == 'no-violation':\n",
    "                no_violation.append(df.itemid[i])\n",
    "                no_violation = remove_dup(no_violation)\n",
    "    ### make a df and save in each article folder\n",
    "    violation_lst = ['violation'] * len(violation)\n",
    "    no_violation_lst = ['no-violation'] * len(no_violation)\n",
    "    concat_outcome_lst = violation_lst + no_violation_lst ### outcome list\n",
    "    concat_id_lst = violation + no_violation ### case id list\n",
    "    df_outcome = pd.DataFrame(list(zip(concat_id_lst, concat_outcome_lst)), columns = ['Itemid', 'Judgement'])\n",
    "    df_outcome.to_csv(join(output_path, 'case_outcome.csv'.format(article)), index = False)\n",
    "    \n",
    "  # sort judgement documents\n",
    "    violation_folder = join(output_path, 'violation')\n",
    "    no_violation_folder = join(output_path, 'no-violation')\n",
    "    mkdir(violation_folder)\n",
    "    mkdir(no_violation_folder)\n",
    "    sort_docs(doc_folder, violation_folder, violation)\n",
    "    sort_docs(doc_folder, no_violation_folder, no_violation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "\n",
    "[1] A. Quemy and R. Wrembel,\"On Integrating and Classifying Legal Text Documents\", International Conference on Database and Expert Systems Applications (DEXA)(2020)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
